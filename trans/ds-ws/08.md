

# 8ã€‚è¶…å‚æ•°è°ƒè°

æ¦‚è¿°

åœ¨æœ¬ç« ä¸­ï¼Œåœ¨æ¼”ç¤ºä»»ä½•é«˜çº§ scikit-learn å®ç°ä¹‹å‰ï¼Œå°†é¦–å…ˆå°†æ¯ä¸ªè¶…å‚æ•°è°ƒæ•´ç­–ç•¥åˆ†è§£ä¸ºå…¶å…³é”®æ­¥éª¤ã€‚è¿™æ˜¯ä¸ºäº†ç¡®ä¿æ‚¨åœ¨è·³åˆ°æ›´è‡ªåŠ¨åŒ–çš„æ–¹æ³•ä¹‹å‰å®Œå…¨ç†è§£æ¯ç§ç­–ç•¥èƒŒåçš„æ¦‚å¿µã€‚

åœ¨æœ¬ç« ç»“æŸæ—¶ï¼Œä½ å°†èƒ½å¤Ÿé€šè¿‡å¯¹å…·æœ‰ä¸åŒè¶…å‚æ•°çš„ä¼°è®¡é‡çš„ç³»ç»Ÿè¯„ä¼°ï¼Œæ‰¾åˆ°é¢„æµ‹æ€§èƒ½çš„è¿›ä¸€æ­¥æ”¹è¿›ã€‚æ‚¨å°†æˆåŠŸéƒ¨ç½²æ‰‹åŠ¨ã€ç½‘æ ¼å’Œéšæœºæœç´¢ç­–ç•¥æ¥æ‰¾åˆ°æœ€ä½³è¶…å‚æ•°ã€‚æ‚¨å°†èƒ½å¤Ÿå‚æ•°åŒ– k-æœ€è¿‘é‚»(k-NN)ã€æ”¯æŒå‘é‡æœº(SVM)ã€å²­å›å½’å’Œéšæœºæ£®æ—åˆ†ç±»å™¨æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚

# ç®€ä»‹

åœ¨å‰å‡ ç« ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†å‡ ç§æ–¹æ³•æ¥å¾—åˆ°ä¸€ä¸ªæ€§èƒ½è‰¯å¥½çš„æ¨¡å‹ã€‚è¿™äº›åŒ…æ‹¬é€šè¿‡é¢„å¤„ç†ã€ç‰¹å¾å·¥ç¨‹å’Œç¼©æ”¾æ¥è½¬æ¢æ•°æ®ï¼Œæˆ–è€…ç®€å•åœ°ä» scikit-learn ç”¨æˆ·å¯ç”¨çš„å¤§é‡å¯èƒ½çš„ä¼°è®¡å™¨ä¸­é€‰æ‹©é€‚å½“çš„ä¼°è®¡å™¨(ç®—æ³•)ç±»å‹ã€‚

æ ¹æ®æ‚¨æœ€ç»ˆé€‰æ‹©çš„ä¼°è®¡å™¨ï¼Œå¯èƒ½æœ‰ä¸€äº›è®¾ç½®å¯ä»¥è°ƒæ•´ï¼Œä»¥æé«˜æ•´ä½“é¢„æµ‹æ€§èƒ½ã€‚è¿™äº›è®¾ç½®ç§°ä¸ºè¶…å‚æ•°ï¼Œå¯¼å‡ºæœ€ä½³è¶…å‚æ•°ç§°ä¸ºè°ƒæ•´æˆ–ä¼˜åŒ–ã€‚é€‚å½“åœ°è°ƒæ•´è¶…å‚æ•°å¯ä»¥å°†æ€§èƒ½æé«˜åˆ°ä¸¤ä½æ•°çš„ç™¾åˆ†æ¯”ï¼Œå› æ­¤åœ¨ä»»ä½•å»ºæ¨¡ç»ƒä¹ ä¸­éƒ½å€¼å¾—ä¸€è¯•ã€‚

æœ¬ç« å°†è®¨è®ºè¶…å‚æ•°è°ƒæ•´çš„æ¦‚å¿µï¼Œå¹¶ç»™å‡ºä¸€äº›ç®€å•çš„ç­–ç•¥ï¼Œä½ å¯ä»¥ç”¨å®ƒä»¬æ¥å¸®åŠ©ä½ çš„ä¼°å€¼å™¨æ‰¾åˆ°æœ€ä½³çš„è¶…å‚æ•°ã€‚

åœ¨å‰é¢çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°äº†ä¸€äº›ä½¿ç”¨ä¸€ç³»åˆ—ä¼°è®¡é‡çš„ç»ƒä¹ ï¼Œä½†æ˜¯æˆ‘ä»¬æ²¡æœ‰è¿›è¡Œä»»ä½•è¶…å‚æ•°è°ƒæ•´ã€‚è¯»å®Œè¿™ä¸€ç« åï¼Œæˆ‘ä»¬å»ºè®®ä½ é‡æ¸©è¿™äº›ç»ƒä¹ ï¼Œåº”ç”¨æ‰€æ•™æˆçš„æŠ€å·§ï¼Œçœ‹çœ‹ä½ æ˜¯å¦èƒ½æé«˜ç»“æœã€‚

# ä»€ä¹ˆæ˜¯è¶…å‚æ•°ï¼Ÿ

è¶…å‚æ•°å¯ä»¥è¢«è®¤ä¸ºæ˜¯æ¯ä¸ªä¼°è®¡é‡çš„ä¸€ç»„æ‹¨å·ç›˜å’Œå¼€å…³ï¼Œç”¨äºæ”¹å˜ä¼°è®¡é‡å¦‚ä½•è§£é‡Šæ•°æ®ä¸­çš„å…³ç³»ã€‚

çœ‹çœ‹*å›¾ 8.1* :

![Figure 8.1: How hyperparameters work
](image/B15019_08_01.jpg)

å›¾ 8.1:è¶…å‚æ•°å¦‚ä½•å·¥ä½œ

å¦‚æœæ‚¨åœ¨ä¸Šå›¾ä¸­ä»å·¦å‘å³é˜…è¯»ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°åœ¨è°ƒæ•´è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ”¹å˜äº†è¶…å‚æ•°çš„å€¼ï¼Œè¿™å¯¼è‡´äº†ä¼°è®¡é‡çš„æ”¹å˜ã€‚è¿™è¿›è€Œå¯¼è‡´æ¨¡å‹æ€§èƒ½çš„å˜åŒ–ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°å¯¼è‡´æœ€ä½³æ¨¡å‹æ€§èƒ½çš„è¶…å‚æ•°åŒ–ã€‚è¿™å°†æ˜¯æœ€ä½³çš„è¶…å‚æ•°åŒ–ã€‚

ä¼°è®¡é‡å¯ä»¥æœ‰ä¸åŒæ•°é‡å’Œç±»å‹çš„è¶…å‚æ•°ï¼Œè¿™æ„å‘³ç€æœ‰æ—¶æ‚¨å¯èƒ½é¢ä¸´å¤§é‡å¯èƒ½çš„è¶…å‚æ•°æ¥é€‰æ‹©ä¼°è®¡é‡ã€‚

ä¾‹å¦‚ï¼Œscikit-learn å®ç°çš„ SVM åˆ†ç±»å™¨(`sklearn.svm.SVC`)æ˜¯ä¸€ä¸ªå…·æœ‰å¤šä¸ªå¯èƒ½çš„è¶…å‚æ•°åŒ–çš„ä¼°è®¡å™¨ï¼Œæˆ‘ä»¬å°†åœ¨æœ¬ç« çš„åé¢ä»‹ç»å®ƒã€‚æˆ‘ä»¬å°†åªæµ‹è¯•å…¶ä¸­çš„ä¸€å°éƒ¨åˆ†ï¼Œå³ä½¿ç”¨ 2ã€3 æˆ– 4 æ¬¡çº¿æ€§æ ¸æˆ–å¤šé¡¹å¼æ ¸ã€‚

è¿™äº›è¶…å‚æ•°ä¸­çš„ä¸€äº›åœ¨æœ¬è´¨ä¸Šæ˜¯è¿ç»­çš„ï¼Œè€Œå¦ä¸€äº›æ˜¯ç¦»æ•£çš„ï¼Œå¹¶ä¸”è¿ç»­è¶…å‚æ•°çš„å­˜åœ¨æ„å‘³ç€å¯èƒ½çš„è¶…å‚æ•°åŒ–çš„æ•°é‡åœ¨ç†è®ºä¸Šæ˜¯æ— é™çš„ã€‚å½“ç„¶ï¼Œå½“æ¶‰åŠåˆ°ç”Ÿæˆå…·æœ‰è‰¯å¥½é¢„æµ‹æ€§èƒ½çš„æ¨¡å‹æ—¶ï¼Œä¸€äº›è¶…å‚æ•°åŒ–è¦æ¯”å…¶ä»–è¶…å‚æ•°åŒ–å¥½å¾—å¤šï¼Œè€Œä½œä¸ºæ•°æ®ç§‘å­¦å®¶ï¼Œæ‚¨çš„å·¥ä½œå°±æ˜¯æ‰¾åˆ°å®ƒä»¬ã€‚

åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ›´è¯¦ç»†åœ°æŸ¥çœ‹è¿™äº›è¶…å‚æ•°çš„è®¾ç½®ã€‚ä½†é¦–å…ˆï¼Œä¸€äº›æœ¯è¯­çš„æ¾„æ¸…ã€‚

## è¶…å‚æ•°å’Œç»Ÿè®¡æ¨¡å‹å‚æ•°ä¹‹é—´çš„å·®å¼‚

åœ¨é˜…è¯»æ•°æ®ç§‘å­¦ï¼Œå°¤å…¶æ˜¯ç»Ÿè®¡å­¦é¢†åŸŸçš„å†…å®¹æ—¶ï¼Œæ‚¨ä¼šé‡åˆ°è¯¸å¦‚â€œæ¨¡å‹å‚æ•°â€ã€â€œå‚æ•°ä¼°è®¡â€å’Œ(éå‚æ•°æ¨¡å‹)ç­‰æœ¯è¯­ã€‚è¿™äº›æœ¯è¯­ä¸æ¨¡å‹æ•°å­¦å…¬å¼ä¸­çš„å‚æ•°æœ‰å…³ã€‚æœ€ç®€å•çš„ä¾‹å­æ˜¯æ²¡æœ‰æˆªè·é¡¹çš„å•å˜é‡çº¿æ€§æ¨¡å‹ï¼Œå…¶å½¢å¼å¦‚ä¸‹:

![Figure 8.2: Equation for a single variable linear model
](image/B15019_08_02.jpg)

å›¾ 8.2:å•å˜é‡çº¿æ€§æ¨¡å‹çš„æ–¹ç¨‹

åœ¨è¿™é‡Œï¼Œğ›½æ˜¯ç»Ÿè®¡æ¨¡å‹å‚æ•°ï¼Œå¦‚æœé€‰æ‹©è¿™ä¸ªå…¬å¼ï¼Œé‚£ä¹ˆæ•°æ®ç§‘å­¦å®¶çš„å·¥ä½œå°±æ˜¯ä½¿ç”¨æ•°æ®æ¥ä¼°è®¡å®ƒçš„å€¼ã€‚è¿™å¯ä»¥ä½¿ç”¨æ™®é€šçš„æœ€å°äºŒä¹˜(OLS)å›å½’æ¨¡å‹æ¥å®ç°ï¼Œä¹Ÿå¯ä»¥é€šè¿‡ä¸€ç§ç§°ä¸ºä¸­å€¼å›å½’çš„æ–¹æ³•æ¥å®ç°ã€‚

è¶…å‚æ•°æ˜¯ä¸åŒçš„ï¼Œå› ä¸ºå®ƒä»¬åœ¨æ•°å­¦å½¢å¼ä¹‹å¤–ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¶…å‚æ•°çš„ä¸€ä¸ªä¾‹å­æ˜¯ä¼°è®¡ğ›½çš„æ–¹æ³•(OLS æˆ–ä¸­ä½æ•°å›å½’)ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¶…å‚æ•°å¯ä»¥å®Œå…¨æ”¹å˜ç®—æ³•(å³ç”Ÿæˆå®Œå…¨ä¸åŒçš„æ•°å­¦å½¢å¼)ã€‚åœ¨è¿™ä¸€ç« ä¸­ï¼Œä½ ä¼šçœ‹åˆ°è¿™æ ·çš„ä¾‹å­ã€‚

åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæ‚¨å°†äº†è§£å¦‚ä½•è®¾ç½®è¶…å‚æ•°ã€‚

## è®¾ç½® g è¶…å‚æ•°

åœ¨*ç¬¬ 7 ç« *ã€*æœºå™¨å­¦ä¹ æ¨¡å‹çš„ä¸€èˆ¬åŒ–*ä¸­ï¼Œæ‚¨äº†è§£äº†ç”¨äºåˆ†ç±»çš„ k-NN æ¨¡å‹ï¼Œå¹¶ä¸”æ‚¨çœ‹åˆ°äº†æ”¹å˜ k(æœ€è¿‘é‚»çš„æ•°é‡)å¦‚ä½•å¯¼è‡´ä¸ç±»åˆ«æ ‡ç­¾é¢„æµ‹ç›¸å…³çš„æ¨¡å‹æ€§èƒ½çš„å˜åŒ–ã€‚è¿™é‡Œï¼Œk æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œæ‰‹åŠ¨å°è¯• k çš„ä¸åŒå€¼çš„è¡Œä¸ºæ˜¯è¶…å‚æ•°è°ƒæ•´çš„ä¸€ç§ç®€å•å½¢å¼ã€‚

æ¯æ¬¡åˆå§‹åŒ– scikit-learn ä¼°è®¡å™¨æ—¶ï¼Œå®ƒéƒ½ä¼šæ ¹æ®æ‚¨ä¸ºå…¶å‚æ•°è®¾ç½®çš„å€¼è¿›è¡Œè¶…å‚æ•°åŒ–ã€‚å¦‚æœæ²¡æœ‰æŒ‡å®šå€¼ï¼Œé‚£ä¹ˆä¼°è®¡å™¨å°†é‡‡ç”¨é»˜è®¤çš„è¶…å‚æ•°åŒ–ã€‚å¦‚æœæ‚¨æƒ³äº†è§£å¦‚ä½•ä¸ºæ‚¨çš„ä¼°è®¡å™¨è®¾ç½®è¶…å‚æ•°ï¼Œä»¥åŠæ‚¨å¯ä»¥è°ƒæ•´å“ªäº›è¶…å‚æ•°ï¼Œåªéœ€æ‰“å°`estimator.get_params()`æ–¹æ³•çš„è¾“å‡ºã€‚

ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬åœ¨æ²¡æœ‰æŒ‡å®šä»»ä½•å‚æ•°(ç©ºæ‹¬å·)çš„æƒ…å†µä¸‹åˆå§‹åŒ–ä¸€ä¸ª k-NN ä¼°è®¡å™¨ã€‚è¦æŸ¥çœ‹é»˜è®¤çš„è¶…å‚æ•°åŒ–ï¼Œæˆ‘ä»¬å¯ä»¥è¿è¡Œ:

```
from sklearn import neighbors
# initialize with default hyperparameters
knn = neighbors.KNeighborsClassifier()
# examine the defaults
print(knn.get_params())
```

æ‚¨åº”è¯¥å¾—åˆ°ä»¥ä¸‹è¾“å‡º:

```
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
```

æ‰€æœ‰è¶…å‚æ•°çš„å­—å…¸ç°åœ¨è¢«æ‰“å°åˆ°å±å¹•ä¸Šï¼Œæ˜¾ç¤ºå®ƒä»¬çš„é»˜è®¤è®¾ç½®ã€‚æ³¨æ„`k`ï¼Œæˆ‘ä»¬æœ€è¿‘é‚»å±…çš„æ•°é‡ï¼Œè¢«è®¾ç½®ä¸º`5`ã€‚

è¦è·å¾—æœ‰å…³è¿™äº›å‚æ•°çš„å«ä¹‰ã€å¦‚ä½•æ›´æ”¹å®ƒä»¬ä»¥åŠå®ƒä»¬å¯èƒ½äº§ç”Ÿçš„å½±å“çš„æ›´å¤šä¿¡æ¯ï¼Œæ‚¨å¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤å¹¶æŸ¥çœ‹æœ‰é—®é¢˜çš„ä¼°è®¡å™¨çš„å¸®åŠ©æ–‡ä»¶ã€‚

å¯¹äºæˆ‘ä»¬çš„ k-NN ä¼°è®¡é‡:

```
?knn
```

è¾“å‡ºå¦‚ä¸‹æ‰€ç¤º:

![Figure 8.3: Help file for the k-NN estimator
](image/B15019_08_03.jpg)

å›¾ 8.3:k-NN ä¼°è®¡å™¨çš„å¸®åŠ©æ–‡ä»¶

å¦‚æœæ‚¨ä»”ç»†æŸ¥çœ‹å¸®åŠ©æ–‡ä»¶ï¼Œæ‚¨ä¼šåœ¨`String form`æ ‡é¢˜ä¸‹çœ‹åˆ°ä¼°è®¡å™¨çš„é»˜è®¤è¶…å‚æ•°åŒ–ï¼Œä»¥åŠåœ¨`Parameters`æ ‡é¢˜ä¸‹å¯¹æ¯ä¸ªè¶…å‚æ•°å«ä¹‰çš„è§£é‡Šã€‚

å›åˆ°æˆ‘ä»¬çš„ä¾‹å­ï¼Œå¦‚æœæˆ‘ä»¬æƒ³å°†è¶…å‚æ•°åŒ–ä»`k = 5`æ›´æ”¹ä¸º`k = 15`ï¼Œåªéœ€é‡æ–°åˆå§‹åŒ–ä¼°è®¡å™¨å¹¶å°†`n_neighbors`å‚æ•°è®¾ç½®ä¸º`15`ï¼Œè¿™å°†è¦†ç›–é»˜è®¤å€¼:

```
# initialize with k = 15 and all other hyperparameters as default 
knn = neighbors.KNeighborsClassifier(n_neighbors=15)
# examine
print(knn.get_params())
```

æ‚¨åº”è¯¥å¾—åˆ°ä»¥ä¸‹è¾“å‡º:

```
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 15, 'p': 2, 'weights': 'uniform'}
```

æ‚¨å¯èƒ½å·²ç»æ³¨æ„åˆ°ï¼Œk ä¸æ˜¯ k-NN åˆ†ç±»å™¨å”¯ä¸€å¯ç”¨çš„è¶…å‚æ•°ã€‚è®¾ç½®å¤šä¸ªè¶…å‚æ•°å°±åƒæŒ‡å®šç›¸å…³å‚æ•°ä¸€æ ·ç®€å•ã€‚ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬å°†é‚»å±…çš„æ•°é‡ä»`5`å¢åŠ åˆ°`15`ï¼Œå¹¶å¼ºåˆ¶ç®—æ³•åœ¨è®­ç»ƒæ—¶è€ƒè™‘é‚»åŸŸä¸­ç‚¹çš„è·ç¦»ï¼Œè€Œä¸æ˜¯ç®€å•çš„å¤šæ•°æŠ•ç¥¨ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§å¸®åŠ©æ–‡ä»¶(`?knn`)ä¸­å¯¹`weights`å‚æ•°çš„æè¿°:

```
# initialize with k = 15, weights = distance and all other hyperparameters as default 
knn = neighbors.KNeighborsClassifier (n_neighbors=15, weights='distance')
# examine
print(knn.get_params())
```

è¾“å‡ºå¦‚ä¸‹æ‰€ç¤º:

```
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 15, 'p': 2, 'weights': 'distance'}
```

åœ¨è¾“å‡ºä¸­ï¼Œä½ å¯ä»¥çœ‹åˆ°`n_neighbors` ( `k`)ç°åœ¨è¢«è®¾ç½®ä¸º`15`ï¼Œ`weights`ç°åœ¨è¢«è®¾ç½®ä¸º`distance`ï¼Œè€Œä¸æ˜¯`uniform`ã€‚

æ³¨æ„

è¿™éƒ¨åˆ†çš„ä»£ç å¯ä»¥åœ¨[https://packt.live/2tN5CH1](https://packt.live/2tN5CH1)æ‰¾åˆ°ã€‚

## å…³äºè¿çº¦çš„è¯´æ˜

é€šå¸¸ï¼Œæœºå™¨å­¦ä¹ åº“çš„å¼€å‘è€…å·²ç»åŠªåŠ›ä¸ºä¼°è®¡å™¨è®¾ç½®åˆç†çš„é»˜è®¤è¶…å‚æ•°ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¯¹äºæŸäº›æ•°æ®é›†ï¼Œé€šè¿‡è°ƒä¼˜å¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚

# å¯»æ‰¾æœ€ä½³è¶…å‚æ•°åŒ–

æœ€ä½³çš„è¶…å‚æ•°åŒ–é¦–å…ˆå–å†³äºä½ å»ºç«‹æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€»ä½“ç›®æ ‡ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œè¿™æ˜¯ä¸ºäº†æ‰¾åˆ°å¯¹çœ‹ä¸è§çš„æ•°æ®å…·æœ‰æœ€é«˜é¢„æµ‹æ€§èƒ½çš„æ¨¡å‹ï¼Œé€šè¿‡å…¶æ­£ç¡®æ ‡è®°æ•°æ®ç‚¹(åˆ†ç±»)æˆ–é¢„æµ‹æ•°å­—(å›å½’)çš„èƒ½åŠ›æ¥è¡¡é‡ã€‚

å¯ä»¥ä½¿ç”¨ä¿ç•™æµ‹è¯•é›†æˆ–äº¤å‰éªŒè¯æ¥æ¨¡æ‹Ÿå¯¹æœªçŸ¥æ•°æ®çš„é¢„æµ‹ï¼Œå‰è€…æ˜¯æœ¬ç« ä¸­ä½¿ç”¨çš„æ–¹æ³•ã€‚åœ¨æ¯ç§æƒ…å†µä¸‹ï¼Œå¯¹æ€§èƒ½è¿›è¡Œä¸åŒçš„è¯„ä¼°ï¼Œä¾‹å¦‚ï¼Œå›å½’çš„å‡æ–¹è¯¯å·®(MSE)å’Œåˆ†ç±»çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬å¯»æ±‚é™ä½ MSE æˆ–è€…å¢åŠ æˆ‘ä»¬é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚

è®©æˆ‘ä»¬åœ¨ä¸‹é¢çš„ç»ƒä¹ ä¸­å®ç°æ‰‹åŠ¨è¶…å‚æ•°åŒ–ã€‚

## ç»ƒä¹ e 8.01:k-NN åˆ†ç±»å™¨çš„æ‰‹åŠ¨è¶…å‚æ•°è°ƒæ•´

åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œæˆ‘ä»¬å°†æ‰‹åŠ¨è°ƒæ•´ä¸€ä¸ª k-NN åˆ†ç±»å™¨ï¼Œè¿™åœ¨*ç¬¬ 7 ç« â€œæœºå™¨å­¦ä¹ æ¨¡å‹çš„æ¨å¹¿â€*ä¸­æœ‰æ‰€ä»‹ç»ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ ¹æ®æ¥è‡ªå—å½±å“ä¹³æˆ¿æ ·æœ¬çš„ç»†èƒæµ‹é‡ç»“æœé¢„æµ‹æ¶æ€§æˆ–è‰¯æ€§ä¹³è…ºç™Œçš„å‘ç—…ç‡ã€‚

æ³¨æ„

æœ¬ç»ƒä¹ ä¸­ä½¿ç”¨çš„æ•°æ®é›†å¯ä»¥åœ¨æˆ‘ä»¬ä½äº https://packt.live/36dsxIF çš„ GitHub å­˜å‚¨åº“ä¸­æ‰¾åˆ°ã€‚

è¿™äº›æ˜¯æ•°æ®é›†çš„é‡è¦å±æ€§:

*   è¯†åˆ«å·
*   è¯Šæ–­(M =æ¶æ€§ï¼ŒB =è‰¯æ€§)
*   3-32)

å¦‚ä¸‹è®¡ç®—æ¯ä¸ªç»†èƒæ ¸çš„ 10 ä¸ªå®å€¼ç‰¹å¾:

*   åŠå¾„(ä»ä¸­å¿ƒåˆ°å‘¨è¾¹å„ç‚¹çš„å¹³å‡è·ç¦»)
*   çº¹ç†(ç°åº¦å€¼çš„æ ‡å‡†åå·®)
*   å‘¨é•¿
*   é¢ç§¯
*   å¹³æ»‘åº¦(åŠå¾„é•¿åº¦çš„å±€éƒ¨å˜åŒ–)
*   å¯†å®åº¦(perimeter^2 /é¢ç§¯- 1.0)
*   å‡¹åº¦(è½®å»“å‡¹é™·éƒ¨åˆ†çš„ä¸¥é‡ç¨‹åº¦)
*   å‡¹ç‚¹(è½®å»“å‡¹é™·éƒ¨åˆ†çš„æ•°é‡)
*   å¯¹ç§°
*   Fractal dimension (refers to the complexity of the tissue architecture; "coastline approximation" - 1)

    æ³¨æ„

    å…³äºæ•°æ®é›†å±æ€§çš„è¯¦ç»†ä¿¡æ¯å¯ä»¥åœ¨[https://packt.live/30HzGQ6](https://packt.live/30HzGQ6)æ‰¾åˆ°ã€‚

ä»¥ä¸‹æ­¥éª¤å°†å¸®åŠ©æ‚¨å®Œæˆæœ¬ç»ƒä¹ :

1.  Create a new notebook in Google Colab.

    æ¥ä¸‹æ¥ï¼Œä» scikit-learn å¯¼å…¥`neighbours`ã€`datasets`å’Œ`model_selection`:

    ```
    from sklearn import neighbors, datasets, model_selection
    ```

2.  åŠ è½½æ•°æ®ã€‚æˆ‘ä»¬å°†è¿™ä¸ªå¯¹è±¡ç§°ä¸º`cancer`ï¼Œå¹¶åˆ†ç¦»å‡ºç›®æ ‡`y`ï¼Œä»¥åŠç‰¹å¾`X` :

    ```
    # dataset
    cancer = datasets.load_breast_cancer()
    # target
    y = cancer.target
    # features
    X = cancer.data
    ```

3.  ç”¨é»˜è®¤çš„è¶…å‚æ•°åŒ–åˆå§‹åŒ– k-NN åˆ†ç±»å™¨:

    ```
    # no arguments specified
    knn = neighbors.KNeighborsClassifier()
    ```

4.  å°†è¯¥åˆ†ç±»å™¨è¾“å…¥ 10 é‡äº¤å‰éªŒè¯(`cv`)ï¼Œè®¡ç®—æ¯ä¸€é‡çš„ç²¾åº¦åˆ†æ•°ã€‚å‡è®¾æœ€å¤§åŒ–ç²¾åº¦(æ‰€æœ‰é˜³æ€§åˆ†ç±»ä¸­çœŸé˜³æ€§çš„æ¯”ä¾‹)æ˜¯æœ¬ç»ƒä¹ çš„ä¸»è¦ç›®æ ‡:

    ```
    # 10 folds, scored on precision
    cv = model_selection.cross_val_score(knn, X, y, cv=10, scoring='precision')
    ```

5.  Printing `cv` shows the precision score calculated for each fold:

    ```
    # precision scores
    print(cv)
    ```

    æ‚¨å°†çœ‹åˆ°ä»¥ä¸‹è¾“å‡º:

    ```
    [0.91891892 0.85365854 0.91666667 0.94736842 0.94594595 0.94444444
     0.97222222 0.91891892 0.96875    0.97142857]
    ```

6.  è®¡ç®—å¹¶æ‰“å°æ‰€æœ‰æŠ˜å çš„å¹³å‡ç²¾åº¦åˆ†æ•°ã€‚è¿™å°†ä½¿æˆ‘ä»¬å¯¹æ¨¡å‹çš„æ•´ä½“æ€§èƒ½æœ‰æ‰€äº†è§£ï¼Œå¦‚ä¸‹é¢çš„ä»£ç ç‰‡æ®µæ‰€ç¤º:

    ```
    # average over all folds
    print(round(cv.mean(), 2))
    ```

7.  You should get the following output:

    ```
    0.94
    ```

    ä½ åº”è¯¥çœ‹åˆ°å¹³å‡åˆ†æ•°æ¥è¿‘ 94%ã€‚è¿™è¿˜èƒ½æ”¹è¿›å—ï¼Ÿ

8.  Run everything again, this time setting hyperparameter `k` to `15`. You can see that the result is actually marginally worse (1% lower):

    ```
    # k = 15
    knn = neighbors.KNeighborsClassifier(n_neighbors=15)
    cv = model_selection.cross_val_score(knn, X, y, cv=10, scoring='precision')
    print(round(cv.mean(), 2))
    ```

    è¾“å‡ºå¦‚ä¸‹æ‰€ç¤º:

    ```
    0.93
    ```

9.  Try again with `k` = `7`, `3`, and `1`. In this case, it seems reasonable that the default value of 5 is the best option. To avoid repetition, you may like to define and call a Python function as follows:

    ```
    def evaluate_knn(k):
      knn = neighbors.KNeighborsClassifier(n_neighbors=k)
      cv = model_selection.cross_val_score(knn, X, y, cv=10, scoring='precision')
      print(round(cv.mean(), 2))
    evaluate_knn(k=7)
    evaluate_knn(k=3)
    evaluate_knn(k=1)
    ```

    è¾“å‡ºå¦‚ä¸‹æ‰€ç¤º:

    ```
    0.93
    0.93
    0.92
    ```

    æ²¡æœ‰ä»€ä¹ˆèƒ½æ‰“è´¥ 94%ã€‚

10.  Let's alter a second hyperparameter. Setting `k = 5`, what happens if we change the k-NN weighing system to depend on distance rather than having uniform weights? Run all code again, this time with the following hyperparameterization:

    ```
    # k =5, weights evaluated using distance
    knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights='distance')
    cv = model_selection.cross_val_score(knn, X, y, cv=10, scoring='precision')print(round(cv.mean(), 2)) 
    ```

    æ€§èƒ½æé«˜äº†å—ï¼Ÿ

11.  æ‚¨åº”è¯¥çœ‹ä¸åˆ°é»˜è®¤è¶…å‚æ•°åŒ–çš„è¿›ä¸€æ­¥æ”¹è¿›ï¼Œå› ä¸ºè¾“å‡ºæ˜¯:

    ```
    0.93
    ```

å› æ­¤ï¼Œæˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé»˜è®¤çš„è¶…å‚æ•°åŒ–æ˜¯æœ€ä¼˜çš„ã€‚

## æ‰‹åŠ¨æœç´¢çš„ä¼˜ç‚¹å’Œç¼ºç‚¹

åœ¨æ‰€æœ‰çš„è¶…å‚æ•°è°ƒæ•´ç­–ç•¥ä¸­ï¼Œæ‰‹åŠ¨è¿‡ç¨‹ç»™ä½ æœ€å¤šçš„æ§åˆ¶ã€‚å½“ä½ ç»å†è¿™ä¸ªè¿‡ç¨‹æ—¶ï¼Œä½ å¯ä»¥æ„Ÿè§‰åˆ°ä½ çš„ä¼°è®¡å™¨åœ¨ä¸åŒçš„è¶…å‚æ•°åŒ–ä¸‹çš„è¡¨ç°ï¼Œè¿™æ„å‘³ç€ä½ å¯ä»¥æ ¹æ®ä½ çš„æœŸæœ›è°ƒæ•´å®ƒä»¬ï¼Œè€Œä¸å¿…å°è¯•å¤§é‡ä¸å¿…è¦çš„å¯èƒ½æ€§ã€‚ç„¶è€Œï¼Œåªæœ‰å½“ä½ æƒ³å°è¯•çš„å¯èƒ½æ€§å¾ˆå°çš„æ—¶å€™ï¼Œè¿™ä¸ªç­–ç•¥æ‰æ˜¯å¯è¡Œçš„ã€‚å½“å¯èƒ½æ€§çš„æ•°é‡è¶…è¿‡å¤§çº¦äº”ä¸ªæ—¶ï¼Œè¿™ç§ç­–ç•¥å°±å˜å¾—å¤ªè€—è´¹äººåŠ›è€Œä¸å®ç”¨äº†ã€‚

åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»ä¸¤ç§ç­–ç•¥æ¥æ›´å¥½åœ°å¤„ç†è¿™ç§æƒ…å†µã€‚

# ä½¿ç”¨ç½‘æ ¼æœç´¢è°ƒæ•´

åœ¨æœºå™¨å­¦ä¹ çš„èƒŒæ™¯ä¸‹ï¼Œç½‘æ ¼æœç´¢æŒ‡çš„æ˜¯ä¸€ç§ä»é¢„å®šä¹‰çš„ä¸€ç»„å¯èƒ½æ€§ä¸­ä¸ºæ‚¨é€‰æ‹©çš„ä¼°è®¡å™¨ç³»ç»Ÿåœ°æµ‹è¯•æ¯ä¸ªè¶…å‚æ•°åŒ–çš„ç­–ç•¥ã€‚æ‚¨å†³å®šç”¨äºè¯„ä¼°æ€§èƒ½çš„æ ‡å‡†ï¼Œä¸€æ—¦æœç´¢å®Œæˆï¼Œæ‚¨å¯ä»¥æ‰‹åŠ¨æ£€æŸ¥ç»“æœå¹¶é€‰æ‹©æœ€ä½³çš„è¶…å‚æ•°åŒ–ï¼Œæˆ–è€…è®©æ‚¨çš„è®¡ç®—æœºè‡ªåŠ¨ä¸ºæ‚¨é€‰æ‹©ã€‚

æ€»ä½“ç›®æ ‡æ˜¯å°è¯•å¹¶æ‰¾åˆ°ä¸€ä¸ªæœ€ä½³çš„è¶…å‚æ•°åŒ–ï¼Œä»è€Œåœ¨é¢„æµ‹æœªçŸ¥æ•°æ®æ—¶æé«˜æ€§èƒ½ã€‚

åœ¨æˆ‘ä»¬å¼€å§‹åœ¨ scikit-learn ä¸­å®ç°ç½‘æ ¼æœç´¢ä¹‹å‰ï¼Œè®©æˆ‘ä»¬é¦–å…ˆä½¿ç”¨ç®€å•çš„ Python `for`å¾ªç¯æ¥æ¼”ç¤ºè¿™ä¸ªç­–ç•¥ã€‚

## ç½‘æ ¼æœç´¢ç­–ç•¥çš„ç®€å•æ¼”ç¤º

åœ¨ä¸‹é¢çš„ç½‘æ ¼æœç´¢ç­–ç•¥æ¼”ç¤ºä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨åœ¨*ç»ƒä¹  8.01* ä¸­çœ‹åˆ°çš„ä¹³è…ºç™Œé¢„æµ‹æ•°æ®é›†ï¼Œå…¶ä¸­æˆ‘ä»¬æ‰‹åŠ¨è°ƒæ•´äº† k-NN åˆ†ç±»å™¨çš„è¶…å‚æ•°ï¼Œä»¥ä¼˜åŒ–ç™Œç—‡é¢„æµ‹çš„ç²¾åº¦ã€‚

è¿™ä¸€æ¬¡ï¼Œæˆ‘ä»¬ä¸æ˜¯ç”¨ä¸åŒçš„`k`å€¼æ¥æ‰‹åŠ¨æ‹Ÿåˆæ¨¡å‹ï¼Œè€Œæ˜¯å®šä¹‰æˆ‘ä»¬æƒ³è¦å°è¯•çš„`k`å€¼ï¼Œä¹Ÿå°±æ˜¯ Python å­—å…¸ä¸­çš„`k = 1, 3, 5, 7`ã€‚è¿™ä¸ªå­—å…¸å°†æ˜¯æˆ‘ä»¬å°†æœç´¢çš„ç½‘æ ¼ï¼Œä»¥æ‰¾åˆ°æœ€ä½³çš„è¶…å‚æ•°åŒ–ã€‚

æ³¨æ„

è¿™éƒ¨åˆ†çš„ä»£ç å¯ä»¥åœ¨[https://packt.live/2U1Y0Li](https://packt.live/2U1Y0Li)æ‰¾åˆ°ã€‚

ä»£ç å¦‚ä¸‹æ‰€ç¤º:

```
from sklearn import neighbors, datasets, model_selection
# load data
cancer = datasets.load_breast_cancer()
# target
y = cancer.target
# features
X = cancer.data
# hyperparameter grid
grid = {
    'k': [1, 3, 5, 7]
}
```

åœ¨ä»£ç ç‰‡æ®µä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å­—å…¸`{}`å¹¶åœ¨ Python å­—å…¸ä¸­è®¾ç½®äº†`k`å€¼ã€‚

åœ¨ä»£ç ç‰‡æ®µçš„ä¸‹ä¸€éƒ¨åˆ†ä¸­ï¼Œä¸ºäº†è¿›è¡Œæœç´¢ï¼Œæˆ‘ä»¬éå†ç½‘æ ¼ï¼Œä¸ºæ¯ä¸ªå€¼`k`æ‹Ÿåˆä¸€ä¸ªæ¨¡å‹ï¼Œæ¯æ¬¡é€šè¿‡ 10 é‡äº¤å‰éªŒè¯æ¥è¯„ä¼°æ¨¡å‹ã€‚

åœ¨æ¯æ¬¡è¿­ä»£ç»“æŸæ—¶ï¼Œæˆ‘ä»¬é€šè¿‡`print`æ–¹æ³•æå–ã€æ ¼å¼åŒ–å¹¶æŠ¥å‘Šäº¤å‰éªŒè¯åçš„å¹³å‡ç²¾åº¦åˆ†æ•°:

```
# for every value of k in the grid
for k in grid['k']:

    # initialize the knn estimator
    knn = neighbors.KNeighborsClassifier(n_neighbors=k)

    # conduct a 10-fold cross-validation
    cv = model_selection.cross_val_score(knn, X, y, cv=10, scoring='precision')

    # calculate the average precision value over all folds
    cv_mean = round(cv.mean(), 3)

    # report the result
    print('With k = {}, mean precision = {}'.format(k, cv_mean))
```

è¾“å‡ºå¦‚ä¸‹æ‰€ç¤º:

![Figure 8.4: Average precisions for all folds
](image/B15019_08_04.jpg)

å›¾ 8.4:æ‰€æœ‰è¤¶çš±çš„å¹³å‡ç²¾åº¦

æˆ‘ä»¬å¯ä»¥ä»è¾“å‡ºä¸­çœ‹åˆ°,`k = 5`æ˜¯æ‰¾åˆ°çš„æœ€å¥½çš„è¶…å‚æ•°åŒ–ï¼Œå¹³å‡ç²¾åº¦åˆ†æ•°å¤§çº¦ä¸º 94%ã€‚å°†`k`å¢åŠ åˆ°`7`å¹¶æ²¡æœ‰æ˜¾è‘—æé«˜æ€§èƒ½ã€‚é‡è¦çš„æ˜¯è¦æ³¨æ„ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œæ”¹å˜çš„å”¯ä¸€å‚æ•°æ˜¯ kï¼Œå¹¶ä¸”æ¯æ¬¡åˆå§‹åŒ– k-NN ä¼°è®¡å™¨æ—¶ï¼Œå…¶ä½™çš„è¶…å‚æ•°éƒ½è®¾ç½®ä¸ºé»˜è®¤å€¼ã€‚

ä¸ºäº†æ˜ç¡®è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥è¿è¡Œç›¸åŒçš„å¾ªç¯ï¼Œè¿™ä¸€æ¬¡åªæ˜¯æ‰“å°å°†è¦å°è¯•çš„è¶…å‚æ•°åŒ–:

```
# for every value of k in the grid 
for k in grid['k']:

    # initialize the knn estimator
    knn = neighbors.KNeighborsClassifier(n_neighbors=k)

    # print the hyperparameterization
    print(knn.get_params())
```

è¾“å‡ºå¦‚ä¸‹æ‰€ç¤º:

```
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 1, 'p': 2, 'weights': 'uniform'}
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 3, 'p': 2, 'weights': 'uniform'}
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 7, 'p': 2, 'weights': 'uniform'}
```

æ‚¨å¯ä»¥ä»è¾“å‡ºä¸­çœ‹åˆ°ï¼Œæˆ‘ä»¬æ›´æ”¹çš„å”¯ä¸€å‚æ•°æ˜¯ kï¼›åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œå…¶ä»–ä¸€åˆ‡éƒ½ä¿æŒä¸å˜ã€‚

ç®€å•çš„å•å¾ªç¯ç»“æ„å¯¹äºå•ä¸ªè¶…å‚æ•°çš„ç½‘æ ¼æœç´¢æ¥è¯´å¾ˆå¥½ï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬æƒ³å°è¯•ç¬¬äºŒç§å‘¢ï¼Ÿè®°ä½ï¼Œå¯¹äº k-NNï¼Œæˆ‘ä»¬ä¹Ÿæœ‰å–å€¼ä¸º`uniform`æˆ–`distance`çš„æƒé‡ï¼Œå®ƒä»¬çš„é€‰æ‹©ä¼šå½±å“ k-NN å­¦ä¹ å¦‚ä½•åˆ†ç±»ç‚¹ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦åšçš„å°±æ˜¯åˆ›å»ºä¸€ä¸ªåŒ…å« k å€¼å’Œæƒé‡å‡½æ•°çš„å­—å…¸ï¼Œæˆ‘ä»¬å¸Œæœ›å°†å®ƒä»¬ä½œä¸ºå•ç‹¬çš„é”®/å€¼å¯¹:

```
# hyperparameter grid
grid = {
    'k': [1, 3, 5, 7],
    'weight_function': ['uniform', 'distance']
}
# for every value of k in the grid
for k in grid['k']:

    # and every possible weight_function in the grid 
    for weight_function in grid['weight_function']:

        # initialize the knn estimator
        knn = neighbors.KNeighborsClassifier(n_neighbors=k, weights=weight_function)

        # conduct a 10-fold cross-validation
        cv = model_selection.cross_val_score(knn, X, y, cv=10, scoring='precision')

        # calculate the average precision value over all folds
        cv_mean = round(cv.mean(), 3)

        # report the result
        print('With k = {} and weight function = {}, mean precision = {}'.format(k, weight_function, cv_mean))
```

è¾“å‡ºå¦‚ä¸‹æ‰€ç¤º:

![Figure 8.5: Average precision values for all folds for different values of k
](image/B15019_08_05.jpg)

å›¾ 8.5:ä¸åŒ k å€¼ä¸‹æ‰€æœ‰æŠ˜å çš„å¹³å‡ç²¾åº¦å€¼

æ‚¨å¯ä»¥çœ‹åˆ°ï¼Œå½“`k = 5`æ—¶ï¼Œæƒé‡å‡½æ•°ä¸åŸºäºè·ç¦»ï¼Œæ‰€æœ‰å…¶ä»–è¶…å‚æ•°ä¿æŒé»˜è®¤å€¼ï¼Œå¹³å‡ç²¾åº¦æœ€é«˜ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æ‰€è®¨è®ºçš„ï¼Œå¦‚æœæ‚¨æƒ³çœ‹åˆ° k-NN çš„å®Œæ•´çš„è¶…å‚æ•°åŒ–é›†ï¼Œåªéœ€åœ¨ä¼°è®¡å™¨åˆå§‹åŒ–ååœ¨`for`å¾ªç¯ä¸­æ·»åŠ `print(knn.get_params())`:

```
# for every value of k in the grid
for k in grid['k']:

  # and every possible weight_function in the grid 
    for weight_function in grid['weight_function']:

        # initialize the knn estimator
        knn = neighbors.KNeighborsClassifier(n_neighbors=k, weights=weight_function)
        # print the hyperparameterizations
        print(knn.get_params())
```

è¾“å‡ºå¦‚ä¸‹æ‰€ç¤º:

```
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 1, 'p': 2, 'weights': 'uniform'}
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 1, 'p': 2, 'weights': 'distance'}
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 3, 'p': 2, 'weights': 'uniform'}
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 3, 'p': 2, 'weights': 'distance'}
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 5, 'p': 2, 'weights': 'distance'}
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 7, 'p': 2, 'weights': 'uniform'}
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 7, 'p': 2, 'weights': 'distance'}
```

è¿™ç§å®ç°è™½ç„¶å¾ˆå¥½åœ°å±•ç¤ºäº†ç½‘æ ¼æœç´¢è¿‡ç¨‹æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œä½†åœ¨è¯•å›¾è¯„ä¼°å…·æœ‰`3`ã€`4`ã€ç”šè‡³`10`ä¸åŒç±»å‹çš„è¶…å‚æ•°(æ¯ä¸ªè¶…å‚æ•°éƒ½æœ‰å¤šç§å¯èƒ½çš„è®¾ç½®)çš„ä¼°è®¡å™¨æ—¶ï¼Œå¯èƒ½å¹¶ä¸å®ç”¨ã€‚

ä»¥è¿™ç§æ–¹å¼ç»§ç»­ä¸‹å»å°†æ„å‘³ç€ç¼–å†™å’Œè·Ÿè¸ªå¤šä¸ª`for`å¾ªç¯ï¼Œè¿™å¯èƒ½æ˜¯ä¹å‘³çš„ã€‚è°¢å¤©è°¢åœ°ï¼Œ`scikit-learn`çš„`model_selection`æ¨¡å—ç»™äº†æˆ‘ä»¬ä¸€ä¸ªå«åš`GridSearchCV`çš„æ–¹æ³•ï¼Œè¿™ä¸ªæ–¹æ³•æ›´åŠ ç”¨æˆ·å‹å¥½ã€‚æˆ‘ä»¬å°†åœ¨å‰é¢çš„ä¸»é¢˜ä¸­è®¨è®ºè¿™ä¸ªé—®é¢˜ã€‚

# GridSearchCV

`GridsearchCV`æ˜¯ä¸€ç§è°ƒæ•´æ–¹æ³•ï¼Œå…¶ä¸­å¯ä»¥é€šè¿‡è¯„ä¼°ç½‘æ ¼ä¸­æåˆ°çš„å‚æ•°ç»„åˆæ¥æ„å»ºæ¨¡å‹ã€‚åœ¨ä¸‹å›¾ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°`GridSearchCV`ä¸æ‰‹åŠ¨æœç´¢æœ‰ä½•ä¸åŒï¼Œå¹¶ä»¥è¡¨æ ¼çš„å½¢å¼æ›´è¯¦ç»†åœ°äº†è§£ç½‘æ ¼æœç´¢ã€‚

## ä½¿ç”¨ GridSearchCV è°ƒä¼˜

åœ¨å®è·µä¸­ï¼Œé€šè¿‡åˆ©ç”¨`model_selection.GridSearchCV`ï¼Œæˆ‘ä»¬å¯ä»¥æ›´å®¹æ˜“åœ°è¿›è¡Œç½‘æ ¼æœç´¢ã€‚

ä¸ºäº†è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸ä¹‹å‰ç›¸åŒçš„ä¹³è…ºç™Œæ•°æ®é›†å’Œ k-NN åˆ†ç±»å™¨:

```
from sklearn import model_selection, datasets, neighbors
# load the data
cancer = datasets.load_breast_cancer()
# target
y = cancer.target
# features
X = cancer.data
```

åŠ è½½æ•°æ®åï¼Œæˆ‘ä»¬éœ€è¦åšçš„ä¸‹ä¸€ä»¶äº‹æ˜¯åˆå§‹åŒ–æˆ‘ä»¬å¸Œæœ›åœ¨ä¸åŒè¶…å‚æ•°åŒ–ä¸‹è¯„ä¼°çš„ä¼°è®¡å™¨çš„ç±»:

```
# initialize the estimator
knn = neighbors.KNeighborsClassifier()
```

ç„¶åæˆ‘ä»¬å®šä¹‰ç½‘æ ¼:

```
# grid contains k and the weight function
grid = {
    'n_neighbors': [1, 3, 5, 7],
    'weights': ['uniform', 'distance']
}
```

ä¸ºäº†å»ºç«‹æœç´¢ï¼Œæˆ‘ä»¬å°†åˆšåˆšåˆå§‹åŒ–çš„ä¼°è®¡å™¨å’Œæˆ‘ä»¬çš„è¶…å‚æ•°ç½‘æ ¼ä¼ é€’ç»™`model_selection.GridSearchCV()`ã€‚æˆ‘ä»¬è¿˜å¿…é¡»æŒ‡å®šä¸€ä¸ªè¯„åˆ†æ ‡å‡†ï¼Œè¯¥æ ‡å‡†å°†ç”¨äºè¯„ä¼°æœç´¢è¿‡ç¨‹ä¸­å°è¯•çš„å„ç§è¶…å‚æ•°åŒ–çš„æ€§èƒ½ã€‚

æœ€åè¦åšçš„æ˜¯é€šè¿‡`cv`å‚æ•°è®¾ç½®ä½¿ç”¨äº¤å‰éªŒè¯çš„æ•°å­—åˆ†å‰²ã€‚æˆ‘ä»¬å°†æ­¤è®¾ç½®ä¸º`10`ï¼Œä»è€Œè¿›è¡Œ 10 é‡äº¤å‰éªŒè¯:

```
# set up the grid search with scoring on precision and number of folds = 10
gscv = model_selection.GridSearchCV(estimator=knn, param_grid=grid, scoring='precision', cv=10)
```

æœ€åä¸€æ­¥æ˜¯é€šè¿‡è¿™ä¸ªå¯¹è±¡çš„`fit()`æ–¹æ³•å‘å…¶æä¾›æ•°æ®ã€‚ä¸€æ—¦å®Œæˆï¼Œç½‘æ ¼æœç´¢è¿‡ç¨‹å°†å¯åŠ¨:

```
# start the search
gscv.fit(X, y)
```

é»˜è®¤æƒ…å†µä¸‹ï¼Œä¸æœç´¢ç›¸å…³çš„ä¿¡æ¯å°†æ‰“å°åˆ°å±å¹•ä¸Šï¼Œä½¿æ‚¨å¯ä»¥çœ‹åˆ°å°†ä¸º k-NN ä¼°è®¡å™¨è¯„ä¼°çš„ç¡®åˆ‡ä¼°è®¡å™¨å‚æ•°åŒ–:

![Figure 8.6: Estimator parameterizations for the k-NN estimator
](image/B15019_08_06.jpg)

å›¾ 8.6:k-NN ä¼°è®¡é‡çš„ä¼°è®¡é‡å‚æ•°åŒ–

ä¸€æ—¦æœç´¢å®Œæˆï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è®¿é—®å’Œæ‰“å°`cv_results_`å±æ€§æ¥æ£€æŸ¥ç»“æœã€‚`cv_results_`æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«å…³äºæ¯ä¸ªè¶…å‚æ•°åŒ–ä¸‹çš„æ¨¡å‹æ€§èƒ½çš„æœ‰ç”¨ä¿¡æ¯ï¼Œä¾‹å¦‚æ‚¨çš„è¯„åˆ†åº¦é‡çš„å¹³å‡æµ‹è¯•é›†å€¼(`mean_test_score`ï¼Œè¶Šä½è¶Šå¥½)ï¼Œå°è¯•çš„è¶…å‚æ•°åŒ–çš„å®Œæ•´åˆ—è¡¨(`params`ï¼Œä»¥åŠä¸`mean_test_score` ( `rank_test_score`)ç›¸å…³çš„æ¨¡å‹æ’åã€‚

æ‰¾åˆ°çš„æœ€ä½³æ¨¡å‹çš„ç­‰çº§ä¸º 1ï¼Œæ¬¡ä½³æ¨¡å‹çš„ç­‰çº§ä¸º 2ï¼Œä¾æ­¤ç±»æ¨ï¼Œå¦‚å›¾ 8.8 ä¸­çš„*æ‰€ç¤ºã€‚é€šè¿‡`mean_fit_time`æŠ¥å‘Šæ¨¡å‹æ‹Ÿåˆæ—¶é—´ã€‚*

è™½ç„¶é€šå¸¸ä¸è€ƒè™‘è¾ƒå°çš„æ•°æ®é›†ï¼Œä½†æ­¤å€¼å¯èƒ½å¾ˆé‡è¦ï¼Œå› ä¸ºåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½ä¼šå‘ç°é€šè¿‡æŸä¸ªè¶…å‚æ•°åŒ–æ¨¡å‹æ€§èƒ½çš„è¾¹é™…å¢åŠ ä¸æ¨¡å‹æ‹Ÿåˆæ—¶é—´çš„æ˜¾è‘—å¢åŠ ç›¸å…³ï¼Œè¿™å–å†³äºæ‚¨å¯ç”¨çš„è®¡ç®—èµ„æºï¼Œå¯èƒ½ä¼šä½¿è¶…å‚æ•°åŒ–ä¸å¯è¡Œï¼Œå› ä¸ºæ‹Ÿåˆæ—¶é—´å¤ªé•¿:

```
# view the results
print(gscv.cv_results_)
```

è¾“å‡ºå¦‚ä¸‹æ‰€ç¤º:

![Figure 8.7: GridsearchCV results
](image/B15019_08_07.jpg)

å›¾ 8.7: GridsearchCV ç»“æœ

åœ¨ä¸‹å›¾ä¸­å¯ä»¥çœ‹åˆ°æ¨¡å‹ç­‰çº§:

![Figure 8.8: Model ranks
](image/B15019_08_08.jpg)

å›¾ 8.8:æ¨¡å‹ç­‰çº§

æ³¨æ„

å‡ºäºæ¼”ç¤ºç›®çš„ï¼Œè¾“å‡ºè¢«æˆªæ–­ã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°å®Œæ•´çš„è¾“å‡º:[https://packt.live/2uD12uP](https://packt.live/2uD12uP)ã€‚

åœ¨è¾“å‡ºä¸­ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸ªå­—å…¸å¯ä»¥å¾ˆå®¹æ˜“åœ°è½¬æ¢æˆ pandas DataFrameï¼Œè¿™ä½¿å¾—ä¿¡æ¯é˜…è¯»èµ·æ¥æ›´åŠ æ¸…æ™°ï¼Œå¹¶å…è®¸æˆ‘ä»¬æœ‰é€‰æ‹©åœ°æ˜¾ç¤ºæˆ‘ä»¬æ„Ÿå…´è¶£çš„æŒ‡æ ‡ã€‚

ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬ä»…å¯¹å‰äº”ä¸ªé«˜æ€§èƒ½æ¨¡å‹çš„æ¯ä¸ªè¶…å‚æ•°åŒ–(`params`)å’Œå¹³å‡äº¤å‰éªŒè¯æµ‹è¯•åˆ†æ•°(`mean_test_score`)æ„Ÿå…´è¶£:

```
import pandas as pd
# convert the results dictionary to a dataframe
results = pd.DataFrame(gscv.cv_results_)
# select just the hyperparameterizations tried, the mean test scores, order by score and show the top 5 models
print(
results.loc[:,['params','mean_test_score']].sort_values('mean_test_score', ascending=False).head(5)
)
```

è¿è¡Œæ­¤ä»£ç ä¼šäº§ç”Ÿä»¥ä¸‹è¾“å‡º:

![Figure 8.9: mean_test_score for top 5 models
](image/B15019_08_09.jpg)

å›¾ 8.9:å‰ 5 åæ¨¡å‹çš„å¹³å‡æµ‹è¯•åˆ†æ•°

æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨ pandas æ¥äº§ç”Ÿå¦‚ä¸‹ç»“æœçš„å¯è§†åŒ–:

```
# visualise the result
results.loc[:,['params','mean_test_score']].plot.barh(x = 'params')
```

è¾“å‡ºå¦‚ä¸‹æ‰€ç¤º:

![Figure 8.10: Using pandas to visualize the output
](image/B15019_08_10.jpg)

å›¾ 8.10:ä½¿ç”¨ pandas æ¥å¯è§†åŒ–è¾“å‡º

æŸ¥çœ‹ä¸Šå›¾æ—¶ï¼Œæ‚¨ä¼šå‘ç°æ‰¾åˆ°çš„æœ€ä½³è¶…å‚æ•°åŒ–æ˜¯åœ¨`n_neighbors = 5`å’Œ`weights = 'uniform'`å¤„ï¼Œå› ä¸ºè¿™ä¼šäº§ç”Ÿæœ€é«˜çš„å¹³å‡æµ‹è¯•åˆ†æ•°(ç²¾åº¦)ã€‚

æ³¨æ„

è¿™éƒ¨åˆ†çš„ä»£ç å¯ä»¥åœ¨[https://packt.live/2uD12uP](https://packt.live/2uD12uP)æ‰¾åˆ°ã€‚

### æ”¯æŒå‘é‡æœº(SVM)åˆ†ç±»å™¨

SVM åˆ†ç±»å™¨åŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªç›‘ç£æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚å®ƒæ˜¯ä¸€ç±»å¸¸ç”¨çš„ä¼°è®¡é‡ï¼Œå¯ç”¨äºäºŒåˆ†ç±»å’Œå¤šåˆ†ç±»ã€‚ä¼—æ‰€å‘¨çŸ¥ï¼Œå®ƒåœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹è¡¨ç°è‰¯å¥½ï¼Œå› æ­¤å®ƒæ˜¯ä¸€ä¸ªå¯é çš„æ¨¡å‹ã€‚ä¸é«˜åº¦è¿­ä»£æˆ–é›†æˆæ–¹æ³•(å¦‚äººå·¥ç¥ç»ç½‘ç»œæˆ–éšæœºæ£®æ—)ç›¸æ¯”ï¼Œå®ƒçš„è®­ç»ƒé€Ÿåº¦ç›¸å¯¹è¾ƒå¿«ï¼Œå¦‚æœè®¡ç®—æœºçš„å¤„ç†èƒ½åŠ›æœ‰é™ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚

å®ƒåˆ©ç”¨ä¸€ç§ç§°ä¸ºæ ¸å‡½æ•°çš„ç‰¹æ®Šæ•°å­¦å…¬å¼è¿›è¡Œé¢„æµ‹ã€‚æ­¤å‡½æ•°å¯ä»¥é‡‡ç”¨å¤šç§å½¢å¼ï¼Œå…¶ä¸­ä¸€äº›å‡½æ•°(å¦‚å¸¦æ¬¡æ•°(å¹³æ–¹ã€ç«‹æ–¹ç­‰)çš„å¤šé¡¹å¼æ ¸å‡½æ•°)å…·æœ‰è‡ªå·±çš„å¯è°ƒå‚æ•°ã€‚

æ”¯æŒå‘é‡æœºå·²ç»è¢«è¯æ˜åœ¨å›¾åƒåˆ†ç±»ä¸­è¡¨ç°è‰¯å¥½ï¼Œè¿™å°†åœ¨ä¸‹é¢çš„ç»ƒä¹ ä¸­çœ‹åˆ°ã€‚

æ³¨æ„

å…³äºæ”¯æŒå‘é‡æœºçš„æ›´å¤šä¿¡æ¯ï¼Œå‚è§ https://packt.live/37iDytwï¼Œä¹Ÿå¯å‚è€ƒ[https://packt.live/38xaPkC](https://packt.live/38xaPkC)ã€‚

## ç»ƒä¹  8.02:SVM çš„ç½‘æ ¼æœç´¢è¶…å‚æ•°è°ƒæ•´

åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ç§ç§°ä¸º SVM åˆ†ç±»å™¨çš„ä¼°è®¡å™¨ï¼Œå¹¶ä½¿ç”¨ç½‘æ ¼æœç´¢ç­–ç•¥è°ƒæ•´å…¶è¶…å‚æ•°ã€‚

æˆ‘ä»¬åœ¨è¿™é‡Œå°†å…³æ³¨çš„ç›‘ç£å­¦ä¹ ç›®æ ‡æ˜¯ä»…åŸºäºå›¾åƒçš„æ‰‹å†™æ•°å­—(0-9)çš„åˆ†ç±»ã€‚æˆ‘ä»¬å°†ä½¿ç”¨çš„æ•°æ®é›†åŒ…å« 1ï¼Œ797 ä¸ªå¸¦æ ‡ç­¾çš„æ‰‹å†™æ•°å­—å›¾åƒã€‚

æ³¨æ„

æœ¬ç»ƒä¹ ä¸­ä½¿ç”¨çš„æ•°æ®é›†å¯ä»¥åœ¨æˆ‘ä»¬ä½äº https://packt.live/2vdbHg9 çš„ GitHub å­˜å‚¨åº“ä¸­æ‰¾åˆ°ã€‚

å…³äºæ•°æ®é›†å±æ€§çš„ç»†èŠ‚å¯ä»¥åœ¨åŸå§‹æ•°æ®é›†çš„ URL ä¸Šæ‰¾åˆ°:[https://packt.live/36cX35b](https://packt.live/36cX35b)ã€‚

è¿™ä¸ªç»ƒä¹ çš„ä»£ç å¯ä»¥åœ¨ https://packt.live/36At2MO æ‰¾åˆ°ã€‚

1.  åœ¨ Google Colab ä¸­åˆ›å»ºæ–°ç¬”è®°æœ¬ã€‚
2.  ä» scikit-learn ä¸­å¯¼å…¥`datasets`ã€`svm`å’Œ`model_selection`:

    ```
    from sklearn import datasets, svm, model_selection
    ```

3.  åŠ è½½æ•°æ®ã€‚æˆ‘ä»¬ç§°è¿™ä¸ªç‰©ä½“ä¸ºå›¾åƒï¼Œç„¶åæˆ‘ä»¬å°†åˆ†ç¦»å‡ºç›®æ ‡`y`å’Œç‰¹å¾`X`ã€‚åœ¨è®­ç»ƒæ­¥éª¤ä¸­ï¼ŒSVM åˆ†ç±»å™¨å°†å­¦ä¹ `y`å¦‚ä½•ä¸`X`ç›¸å…³ï¼Œå¹¶å› æ­¤èƒ½å¤Ÿåœ¨ç»™å®šæ–°çš„`X`å€¼:

    ```
    # load data
    digits = datasets.load_digits()
    # target
    y = digits.target
    # features
    X = digits.data
    ```

    æ—¶é¢„æµ‹æ–°çš„`y`å€¼
4.  Initialize the estimator as a multi-class SVM classifier and set the `gamma` argument to `scale`:

    ```
    # support vector machine classifier
    clr = svm.SVC(gamma='scale')
    ```

    æ³¨æ„

    å…³äºä¼½ç›å‚æ•°çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·è®¿é—®[https://packt.live/2Ga2l79](https://packt.live/2Ga2l79)ã€‚

5.  å®šä¹‰æˆ‘ä»¬çš„ç½‘æ ¼ï¼Œä»¥è¦†ç›–åˆ†ç±»å™¨çš„å››ä¸ªä¸åŒçš„è¶…å‚æ•°åŒ–ï¼Œè¿™å››ä¸ªè¶…å‚æ•°åŒ–å…·æœ‰çº¿æ€§æ ¸å’Œæ¬¡æ•°ä¸º`2`ã€`3,`å’Œ`4`çš„å¤šé¡¹å¼æ ¸ã€‚æˆ‘ä»¬æƒ³çœ‹çœ‹å››ä¸ªè¶…å‚æ•°åŒ–ä¸­çš„å“ªä¸€ä¸ªå¯¼è‡´æ›´å‡†ç¡®çš„é¢„æµ‹:

    ```
    # hyperparameter grid. contains linear and polynomial kernels
    grid = [
      {'kernel': ['linear']},
     {'kernel': ['poly'], 'degree': [2, 3, 4]}
     ]
    ```

6.  ä½¿ç”¨`10`æŠ˜å å’Œå‡†ç¡®åº¦è¯„åˆ†æ ‡å‡†å»ºç«‹ç½‘æ ¼æœç´¢ k æŠ˜å äº¤å‰éªŒè¯ã€‚ç¡®ä¿å®ƒæœ‰æˆ‘ä»¬çš„`grid`å’Œ`estimator`å¯¹è±¡ä½œä¸ºè¾“å…¥:

    ```
    # setting up the grid search to score on accuracy and evaluate over 10 folds
    cv_spec = model_selection.GridSearchCV(estimator=clr, param_grid=grid, scoring='accuracy', cv=10)
    ```

7.  Start the search by providing data to the `.fit()` method. Details of the process, including the hyperparameterizations tried and the scoring method selected, will be printed to the screen:

    ```
    # start the grid search
    cv_spec.fit(X, y)
    ```

    æ‚¨åº”è¯¥ä¼šçœ‹åˆ°ä»¥ä¸‹è¾“å‡º:

    ![Figure 8.11: GridSearch using the fit() method
    ](image/B15019_08_11.jpg)

    å›¾ 8.11:ä½¿ç”¨ã€‚fit()æ–¹æ³•

8.  To examine all of the results, simply print `cv_spec.cv_results_` to the screen. You will see that the results are structured as a dictionary, allowing you to access the information you require using the keys:

    ```
    # what is the available information
    print(cv_spec.cv_results_.keys())
    ```

    æ‚¨å°†çœ‹åˆ°ä»¥ä¸‹ä¿¡æ¯:

    ![Figure 8.12: Results as a dictionary
    ](image/B15019_08_12.jpg)

    å›¾ 8.12:ä½œä¸ºå­—å…¸çš„ç»“æœ

9.  For this exercise, we are primarily concerned with the test-set performance of each distinct hyperparameterization. You can see the first hyperparameterization through `cv_spec.cv_results_['mean_test_score']`, and the second through `cv_spec.cv_results_['params']`.

    è®©æˆ‘ä»¬å°†ç»“æœå­—å…¸è½¬æ¢æˆä¸€ä¸ª`pandas`æ•°æ®å¸§ï¼Œå¹¶æ‰¾åˆ°æœ€ä½³çš„è¶…å‚æ•°åŒ–:

    ```
    import pandas as pd
    # convert the dictionary of results to a pandas dataframe
    results = pd.DataFrame(cv_spec.cv_results_)
    print(
    # show hyperparameterizations
    results.loc[:,['params','mean_test_score']].sort_values('mean_test_score', ascending=False)
    )
    ```

    æ‚¨å°†çœ‹åˆ°ä»¥ä¸‹ç»“æœ:

    ![Figure 8.13: Parameterization results
    ](image/B15019_08_13.jpg)

    å›¾ 8.13:å‚æ•°åŒ–ç»“æœ

10.  æœ€ä½³å®è·µæ˜¯å°†æ‚¨äº§ç”Ÿçš„ä»»ä½•ç»“æœå¯è§†åŒ–ã€‚ä½¿è¿™å˜å¾—å®¹æ˜“ã€‚è¿è¡Œä¸‹é¢çš„ä»£ç æ¥ç”Ÿæˆå¯è§†åŒ–æ•ˆæœ:

    ```
    # visualize the result
    (
        results.loc[:,['params','mean_test_score']]
        .sort_values('mean_test_score', ascending=True)
        .plot.barh(x='params', xlim=(0.8))
    )
    ```

è¾“å‡ºå¦‚ä¸‹æ‰€ç¤º:

![Figure 8.14: Using pandas to visualize the results
](image/B15019_08_14.jpg)

å›¾ 8.14:ä½¿ç”¨ç†ŠçŒ«æ¥å¯è§†åŒ–ç»“æœ

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå…·æœ‰ä¸‰æ¬¡å¤šé¡¹å¼æ ¸å‡½æ•°çš„ SVM åˆ†ç±»å™¨åœ¨æˆ‘ä»¬çš„æœç´¢ä¸­è¯„ä¼°çš„æ‰€æœ‰è¶…å‚æ•°åŒ–ä¸­å…·æœ‰æœ€é«˜çš„å‡†ç¡®æ€§ã€‚è¯·éšæ„å‘ç½‘æ ¼ä¸­æ·»åŠ æ›´å¤šçš„è¶…å‚æ•°åŒ–ï¼Œçœ‹çœ‹æ‚¨æ˜¯å¦å¯ä»¥æé«˜åˆ†æ•°ã€‚

## ç½‘æ ¼æœç´¢çš„ä¼˜ç¼ºç‚¹

ä¸æ‰‹åŠ¨æœç´¢ç›¸æ¯”ï¼Œç½‘æ ¼æœç´¢çš„ä¸»è¦ä¼˜åŠ¿åœ¨äºå®ƒæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„è¿‡ç¨‹ï¼Œäººä»¬å¯ä»¥ç®€å•åœ°è®¾ç½®å¹¶å¿˜è®°ã€‚æ­¤å¤–ï¼Œæ‚¨æœ‰æƒåŠ›æŒ‡å®šæ‰€è¯„ä¼°çš„ç²¾ç¡®è¶…å‚æ•°åŒ–ï¼Œå½“æ‚¨äº‹å…ˆçŸ¥é“å“ªç§è¶…å‚æ•°åŒ–å¯èƒ½åœ¨æ‚¨çš„ç¯å¢ƒä¸­å·¥ä½œè‰¯å¥½æ—¶ï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä»¶å¥½äº‹ã€‚ç”±äºç½‘æ ¼çš„æ˜ç¡®å®šä¹‰ï¼Œä¹Ÿå¾ˆå®¹æ˜“å‡†ç¡®ç†è§£æœç´¢è¿‡ç¨‹ä¸­ä¼šå‘ç”Ÿä»€ä¹ˆã€‚

ç½‘æ ¼æœç´¢ç­–ç•¥çš„ä¸»è¦ç¼ºç‚¹æ˜¯è®¡ç®—é‡éå¸¸å¤§ï¼›ä¹Ÿå°±æ˜¯è¯´ï¼Œå½“è¦å°è¯•çš„è¶…å‚æ•°åŒ–çš„æ•°é‡æ˜¾è‘—å¢åŠ æ—¶ï¼Œå¤„ç†æ—¶é—´å¯èƒ½ä¼šéå¸¸æ…¢ã€‚æ­¤å¤–ï¼Œå½“æ‚¨å®šä¹‰ç½‘æ ¼æ—¶ï¼Œæ‚¨å¯èƒ½ä¼šæ— æ„ä¸­å¿½ç•¥ä¸€ä¸ªå®é™…ä¸Šæ˜¯æœ€ä½³çš„è¶…å‚æ•°åŒ–ã€‚å¦‚æœåœ¨æ‚¨çš„ç½‘æ ¼ä¸­æ²¡æœ‰æŒ‡å®šï¼Œå®ƒå°†æ°¸è¿œä¸ä¼šè¢«å°è¯•

ä¸ºäº†å…‹æœè¿™äº›ç¼ºç‚¹ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚ç ”ç©¶éšæœºæœç´¢ã€‚

# éšæœºæœç´¢

åœ¨éšæœºæœç´¢ä¸­ï¼Œæˆ‘ä»¬å‡è®¾æ¯ä¸ªè¶…å‚æ•°éƒ½æ˜¯éšæœºå˜é‡ï¼Œè€Œä¸æ˜¯åƒç½‘æ ¼æœç´¢é‚£æ ·æœç´¢é¢„å®šä¹‰é›†åˆä¸­çš„æ¯ä¸ªè¶…å‚æ•°ã€‚åœ¨æˆ‘ä»¬æ·±å…¥ç ”ç©¶è¿™ä¸ªè¿‡ç¨‹ä¹‹å‰ï¼Œç®€è¦å›é¡¾ä¸€ä¸‹ä»€ä¹ˆæ˜¯éšæœºå˜é‡ä»¥åŠåˆ†å¸ƒçš„å«ä¹‰æ˜¯æœ‰å¸®åŠ©çš„ã€‚

## éšæœºå˜é‡åŠå…¶åˆ†å¸ƒ

éšæœºå˜é‡æ˜¯éæ’å®šçš„(å®ƒçš„å€¼å¯ä»¥æ”¹å˜),å®ƒçš„å¯å˜æ€§å¯ä»¥ç”¨åˆ†å¸ƒæ¥æè¿°ã€‚æœ‰è®¸å¤šä¸åŒç±»å‹çš„åˆ†å¸ƒï¼Œä½†æ¯ä¸€ç§éƒ½å±äºä¸¤å¤§ç±»ä¹‹ä¸€:ç¦»æ•£åˆ†å¸ƒå’Œè¿ç»­åˆ†å¸ƒã€‚æˆ‘ä»¬ä½¿ç”¨ç¦»æ•£åˆ†å¸ƒæ¥æè¿°éšæœºå˜é‡ï¼Œå…¶å€¼åªèƒ½å–æ•´æ•°ï¼Œå¦‚è®¡æ•°ã€‚

ä¸€ä¸ªä¾‹å­æ˜¯ä¸€ä¸ªä¸»é¢˜å…¬å›­ä¸€å¤©çš„æ¸¸å®¢æ•°é‡ï¼Œæˆ–è€…ä¸€ä¸ªé«˜å°”å¤«çƒæ‰‹ä¸€æ†è¿›æ´çš„å°è¯•æ¬¡æ•°ã€‚

æˆ‘ä»¬ä½¿ç”¨è¿ç»­åˆ†å¸ƒæ¥æè¿°éšæœºå˜é‡ï¼Œå…¶å€¼ä½äºç”±æ— é™å°çš„å¢é‡ç»„æˆçš„è¿ç»­ç»Ÿä¸Šã€‚ä¾‹å­åŒ…æ‹¬äººçš„èº«é«˜æˆ–ä½“é‡ï¼Œæˆ–å®¤å¤–æ°”æ¸©ã€‚åˆ†å¸ƒé€šå¸¸å…·æœ‰æ§åˆ¶å…¶å½¢çŠ¶çš„å‚æ•°ã€‚

ç¦»æ•£åˆ†å¸ƒå¯ä»¥ç”¨æ‰€è°“çš„æ¦‚ç‡è´¨é‡å‡½æ•°è¿›è¡Œæ•°å­¦æè¿°ï¼Œå®ƒå®šä¹‰äº†éšæœºå˜é‡å–æŸä¸ªå€¼çš„ç¡®åˆ‡æ¦‚ç‡ã€‚è¿™ä¸ªå‡½æ•°å·¦è¾¹å¸¸è§çš„ç¬¦å·æ˜¯`P(X=x)`ï¼Œé€šä¿—åœ°è¯´å°±æ˜¯éšæœºå˜é‡`X`ç­‰äºæŸä¸ªå€¼`x`çš„æ¦‚ç‡æ˜¯`P`ã€‚è®°ä½æ¦‚ç‡çš„èŒƒå›´åœ¨`0`(ä¸å¯èƒ½)å’Œ`1`(ç¡®å®š)ä¹‹é—´ã€‚

æ ¹æ®å®šä¹‰ï¼Œæ‰€æœ‰å¯èƒ½çš„`x`çš„æ¯ä¸ª`P(X=x)`çš„æ€»å’Œå°†ç­‰äº 1ï¼Œæˆ–è€…å¦‚æœç”¨å¦ä¸€ç§æ–¹å¼è¡¨è¾¾ï¼Œ`X`å–ä»»ä½•å€¼çš„æ¦‚ç‡æ˜¯ 1ã€‚è¿™ç§åˆ†å¸ƒçš„ä¸€ä¸ªç®€å•ä¾‹å­æ˜¯ç¦»æ•£å‡åŒ€åˆ†å¸ƒï¼Œå…¶ä¸­éšæœºå˜é‡`X`å°†åªå–æœ‰é™èŒƒå›´å€¼ä¸­çš„ä¸€ä¸ªï¼Œå¹¶ä¸”å®ƒå–ä»»ä½•ç‰¹å®šå€¼çš„æ¦‚ç‡å¯¹äºæ‰€æœ‰å€¼éƒ½æ˜¯ç›¸åŒçš„ï¼Œå› æ­¤ç§°ä¸ºå‡åŒ€åˆ†å¸ƒã€‚

ä¾‹å¦‚ï¼Œå¦‚æœæœ‰ 10 ä¸ªå¯èƒ½çš„å€¼ï¼Œ`X`æ˜¯ä»»ä½•ç‰¹å®šå€¼çš„æ¦‚ç‡æ­£å¥½æ˜¯ 1/10ã€‚å¦‚æœæœ‰ 6 ä¸ªå¯èƒ½çš„å€¼ï¼Œå¦‚åœ¨æ ‡å‡† 6 é¢éª°å­çš„æƒ…å†µä¸‹ï¼Œæ¦‚ç‡å°†æ˜¯ 1/6ï¼Œä¾æ­¤ç±»æ¨ã€‚ç¦»æ•£å‡åŒ€åˆ†å¸ƒçš„æ¦‚ç‡è´¨é‡å‡½æ•°ä¸º:

![Figure 8.15: Probability mass function for the discrete uniform distribution
](image/B15019_08_15.jpg)

å›¾ 8.15:ç¦»æ•£å‡åŒ€åˆ†å¸ƒçš„æ¦‚ç‡è´¨é‡å‡½æ•°

ä¸‹é¢çš„ä»£ç å°†å…è®¸æˆ‘ä»¬çœ‹åˆ° x æœ‰ 10 ä¸ªå¯èƒ½å€¼çš„åˆ†å¸ƒå½¢å¼ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ—å‡ºäº†`X`å¯èƒ½å–çš„æ‰€æœ‰å€¼:

```
# list of all xs
X = list(range(1, 11))
print(X)
```

è¾“å‡ºå¦‚ä¸‹æ‰€ç¤º:

```
 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
```

ç„¶åï¼Œæˆ‘ä»¬è®¡ç®—`X`å°†å ç”¨`x (P(X=x))`çš„ä»»ä½•å€¼çš„æ¦‚ç‡:

```
# pmf, 1/n * n = 1
p_X_x = [1/len(X)] * len(X)
# sums to 1
print(p_X_x)
```

å¦‚å‰æ‰€è¿°ï¼Œæ¦‚ç‡çš„æ€»å’Œç­‰äº 1ï¼Œä»»ä½•åˆ†å¸ƒéƒ½æ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬ç°åœ¨æœ‰äº†å¯è§†åŒ–åˆ†å¸ƒæ‰€éœ€çš„ä¸€åˆ‡:

```
import matplotlib.pyplot as plt
plt.bar(X, p_X_x)
plt.xlabel('X')
plt.ylabel('P(X=x)')
```

è¾“å‡ºå¦‚ä¸‹æ‰€ç¤º:

![Figure 8.16: Visualizing the bar chart
](image/B15019_08_16.jpg)

å›¾ 8.16:å¯è§†åŒ–æ¡å½¢å›¾

åœ¨è§†è§‰è¾“å‡ºä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°`X`æ˜¯ 1 åˆ° 10 ä¹‹é—´çš„ç‰¹å®šæ•´æ•°çš„æ¦‚ç‡ç­‰äº 1/10ã€‚

æ³¨æ„

å…¶ä»–å¸¸è§çš„ç¦»æ•£åˆ†å¸ƒåŒ…æ‹¬äºŒé¡¹åˆ†å¸ƒã€è´ŸäºŒé¡¹åˆ†å¸ƒã€å‡ ä½•åˆ†å¸ƒå’Œæ³Šæ¾åˆ†å¸ƒï¼Œæˆ‘ä»¬é¼“åŠ±æ‚¨ç ”ç©¶æ‰€æœ‰è¿™äº›åˆ†å¸ƒã€‚åœ¨æœç´¢å¼•æ“ä¸­é”®å…¥è¿™äº›æœ¯è¯­å¯ä»¥æ‰¾åˆ°æ›´å¤šä¿¡æ¯ã€‚

è¿ç»­éšæœºå˜é‡çš„åˆ†å¸ƒæ›´å…·æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæˆ‘ä»¬ä¸èƒ½ç›´æ¥è®¡ç®—ç²¾ç¡®çš„`P(X=x)`ï¼Œå› ä¸º`X`ä½äºè¿ç»­ç»Ÿä¸Šã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ç§¯åˆ†æ¥è¿‘ä¼¼ä¸€ç³»åˆ—å€¼ä¹‹é—´çš„æ¦‚ç‡ï¼Œä½†è¿™è¶…å‡ºäº†æœ¬ä¹¦çš„èŒƒå›´ã€‚ä½¿ç”¨æ¦‚ç‡å¯†åº¦å‡½æ•°`P(X)`æè¿°`X`å’Œæ¦‚ç‡ä¹‹é—´çš„å…³ç³»ã€‚ä¹Ÿè®¸æœ€å¹¿ä¸ºäººçŸ¥çš„è¿ç»­åˆ†å¸ƒæ˜¯æ­£æ€åˆ†å¸ƒï¼Œå®ƒåœ¨è§†è§‰ä¸Šè¡¨ç°ä¸ºé’Ÿå½¢ã€‚

æ­£æ€åˆ†å¸ƒæœ‰ä¸¤ä¸ªæè¿°å…¶å½¢çŠ¶çš„å‚æ•°ï¼Œå‡å€¼(`ğœ‡`)å’Œæ–¹å·®(`ğœ` 2)ã€‚æ­£æ€åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°ä¸º:

![Figure 8.17: Probability density function for the normal distribution
](image/B15019_08_17.jpg)

å›¾ 8.17:æ­£æ€åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°

ä¸‹é¢çš„ä»£ç å±•ç¤ºäº†ä¸¤ä¸ªå‡å€¼(`ğœ‡` `= 0`)ç›¸åŒä½†æ–¹å·®å‚æ•°(`ğœ``2 = 1``ğœ``2 = 2.25`)ä¸åŒçš„æ­£æ€åˆ†å¸ƒã€‚è®©æˆ‘ä»¬é¦–å…ˆä½¿ç”¨ NumPy çš„`.linspace`æ–¹æ³•ç”Ÿæˆä»`-10`åˆ°`10`çš„ 100 ä¸ªç­‰è·å€¼:

```
import numpy as np
# range of xs
x = np.linspace(-10, 10, 100)
```

ç„¶åï¼Œæˆ‘ä»¬ä¸ºä¸¤ä¸ªæ­£æ€åˆ†å¸ƒç”Ÿæˆè¿‘ä¼¼çš„`X`æ¦‚ç‡ã€‚

ä½¿ç”¨`scipy.stats`æ˜¯ä¸€ç§å¤„ç†åˆ†å¸ƒçš„å¥½æ–¹æ³•ï¼Œå®ƒçš„`pdf`æ–¹æ³•å¯ä»¥è®©æˆ‘ä»¬å¾ˆå®¹æ˜“åœ°å¯è§†åŒ–æ¦‚ç‡å¯†åº¦å‡½æ•°çš„å½¢çŠ¶:

```
import scipy.stats as stats
# first normal distribution with mean = 0, variance = 1
p_X_1 = stats.norm.pdf(x=x, loc=0.0, scale=1.0**2)
# second normal distribution with mean = 0, variance = 2.25
p_X_2 = stats.norm.pdf(x=x, loc=0.0, scale=1.5**2)
```

æ³¨æ„

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ`loc`å¯¹åº”äºğœ‡ï¼Œè€Œ`scale`å¯¹åº”äºæ ‡å‡†å·®ï¼Œæ ‡å‡†å·®æ˜¯`ğœ` `2`çš„å¹³æ–¹æ ¹ï¼Œå› æ­¤æˆ‘ä»¬å¯¹è¾“å…¥æ±‚å¹³æ–¹ã€‚

ç„¶åæˆ‘ä»¬æƒ³è±¡ç»“æœã€‚è¯·æ³¨æ„ï¼Œ`ğœ` `2`æ§åˆ¶åˆ†å¸ƒçš„å®½åº¦ï¼Œä»è€Œæ§åˆ¶éšæœºå˜é‡çš„å¯å˜æ€§:

```
plt.plot(x,p_X_1, color='blue')
plt.plot(x, p_X_2, color='orange')
plt.xlabel('X')
plt.ylabel('P(X)')
```

è¾“å‡ºå¦‚ä¸‹æ‰€ç¤º:

![Figure 8.18: Visualizing the normal distribution
](image/B15019_08_18.jpg)

å›¾ 8.18:å¯è§†åŒ–æ­£æ€åˆ†å¸ƒ

æ‚¨é€šå¸¸çœ‹åˆ°çš„å…¶ä»–ç¦»æ•£åˆ†å¸ƒåŒ…æ‹¬ä¼½ç›ã€æŒ‡æ•°å’Œè´å¡”åˆ†å¸ƒï¼Œæˆ‘ä»¬é¼“åŠ±æ‚¨ç ”ç©¶è¿™äº›åˆ†å¸ƒã€‚

æ³¨æ„

è¿™éƒ¨åˆ†çš„ä»£ç å¯ä»¥åœ¨[https://packt.live/38Mfyzm](https://packt.live/38Mfyzm)æ‰¾åˆ°ã€‚

## Rando m æœç´¢è¿‡ç¨‹çš„ç®€å•æ¼”ç¤º

åŒæ ·ï¼Œåœ¨æˆ‘ä»¬å¼€å§‹éšæœºæœç´¢å‚æ•°è°ƒä¼˜çš„ scikit-learn å®ç°ä¹‹å‰ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç®€å•çš„ Python å·¥å…·é€æ­¥å®Œæˆè¿™ä¸ªè¿‡ç¨‹ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬åªæ˜¯ä½¿ç”¨åˆ†ç±»é—®é¢˜æ¥æ¼”ç¤ºè°ƒä¼˜æ¦‚å¿µï¼Œä½†æ˜¯ç°åœ¨æˆ‘ä»¬å°†ç ”ç©¶ä¸€ä¸ªå›å½’é—®é¢˜ã€‚æˆ‘ä»¬èƒ½å¦æ‰¾åˆ°ä¸€ç§æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®èº«ä½“è´¨é‡æŒ‡æ•°å’Œå¹´é¾„ç­‰ç‰¹å¾é¢„æµ‹æ‚£è€…çš„ç³–å°¿ç—…è¿›å±•ï¼Ÿ

æ³¨æ„

åŸå§‹æ•°æ®é›†å¯ä»¥åœ¨[https://packt.live/2O4XN6v](https://packt.live/2O4XN6v)æ‰¾åˆ°ã€‚

è¿™éƒ¨åˆ†çš„ä»£ç å¯ä»¥åœ¨[https://packt.live/3aOudvK](https://packt.live/3aOudvK)æ‰¾åˆ°ã€‚

æˆ‘ä»¬é¦–å…ˆåŠ è½½æ•°æ®:

```
from sklearn import datasets, linear_model, model_selection
# load the data
diabetes = datasets.load_diabetes()
# target
y = diabetes.target
# features
X = diabetes.data
```

ä¸ºäº†å¯¹æ•°æ®æœ‰æ‰€äº†è§£ï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥ç¬¬ä¸€ä¸ªæ‚£è€…çš„ç–¾ç—…è¿›å±•:

```
# the first patient has index 0
print(y[0])
```

è¾“å‡ºå¦‚ä¸‹æ‰€ç¤º:

```
 151.0
```

ç°åœ¨è®©æˆ‘ä»¬æ¥çœ‹çœ‹å®ƒä»¬çš„ç‰¹å¾:

```
# let's look at the first patients data
print(
  dict(zip(diabetes.feature_names, X[0]))
)
```

æˆ‘ä»¬åº”è¯¥çœ‹åˆ°ä»¥ä¸‹å†…å®¹:

![Figure 8.19: Dictionary for patient characteristics
](image/B15019_08_19.jpg)

å›¾ 8.19:æ‚£è€…ç‰¹å¾å­—å…¸

å¯¹äºè¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬å°†å°è¯•ä¸€ç§ç§°ä¸ºå²­å›å½’çš„æŠ€æœ¯ï¼Œå®ƒå°†çº¿æ€§æ¨¡å‹æ‹Ÿåˆåˆ°æ•°æ®ä¸­ã€‚å²­å›å½’æ˜¯ä¸€ç§ç‰¹æ®Šçš„æ–¹æ³•ï¼Œå®ƒå…è®¸æˆ‘ä»¬ç›´æ¥ä½¿ç”¨æ­£åˆ™åŒ–æ¥å¸®åŠ©å‡è½»è¿‡åº¦æ‹Ÿåˆçš„é—®é¢˜ã€‚å²­å›å½’æœ‰ä¸€ä¸ªå…³é”®çš„è¶…å‚æ•°ğ›¼ï¼Œå®ƒæ§åˆ¶ç€æ¨¡å‹æ‹Ÿåˆä¸­çš„æ­£åˆ™åŒ–ç¨‹åº¦ã€‚å¦‚æœğ›¼è®¾ç½®ä¸º 1ï¼Œå°†ä¸ä¼šä½¿ç”¨ä»»ä½•æ­£åˆ™åŒ–ï¼Œè¿™å®é™…ä¸Šæ˜¯ä¸€ç§ç‰¹æ®Šæƒ…å†µï¼Œå…¶ä¸­å²­å›å½’æ¨¡å‹æ‹Ÿåˆå°†å®Œå…¨ç­‰äº OLS çº¿æ€§å›å½’æ¨¡å‹çš„æ‹Ÿåˆã€‚å¢åŠ ğ›¼çš„ä»·å€¼ï¼Œä½ å°±å¢åŠ äº†æ­£è§„åŒ–çš„ç¨‹åº¦ã€‚

æ³¨æ„

æˆ‘ä»¬åœ¨*ç¬¬ä¸ƒç« *ã€*ä¸­è®¨è®ºäº†å²­å›å½’å¯¹æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ¨å¹¿*ã€‚

æœ‰å…³å²­å›å½’å’Œæ­£åˆ™åŒ–çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[https://packt.live/2NR3GUq](https://packt.live/2NR3GUq)ã€‚

åœ¨éšæœºæœç´¢å‚æ•°è°ƒæ•´çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œæˆ‘ä»¬å‡è®¾ğ›¼æ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼Œç”±æˆ‘ä»¬æ¥æŒ‡å®šä¸€ä¸ªå¯èƒ½çš„åˆ†å¸ƒã€‚

å¯¹äºè¿™ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å°†å‡è®¾é˜¿å°”æ³•éµå¾ªä¼½ç›åˆ†å¸ƒã€‚è¿™ä¸ªåˆ†å¸ƒæœ‰ä¸¤ä¸ªå‚æ•°ï¼Œk å’Œğœƒï¼Œå®ƒä»¬åˆ†åˆ«æ§åˆ¶åˆ†å¸ƒçš„å½¢çŠ¶å’Œæ¯”ä¾‹ã€‚

å¯¹äºå²­å›å½’ï¼Œæˆ‘ä»¬è®¤ä¸ºæœ€ä½³ğ›¼æ¥è¿‘ 1ï¼Œéšç€è¿œç¦» 1ï¼Œå¯èƒ½æ€§å˜å¾—è¶Šæ¥è¶Šå°ã€‚åæ˜ è¿™ä¸€æ€æƒ³çš„ä¼½é©¬åˆ†å¸ƒçš„å‚æ•°åŒ–æ˜¯ k å’Œğœƒéƒ½ç­‰äº 1ã€‚ä¸ºäº†å½¢è±¡åŒ–è¿™ä¸ªåˆ†å¸ƒçš„å½¢å¼ï¼Œæˆ‘ä»¬å¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤:

```
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
# values of alpha
x = np.linspace(1, 20, 100)
# probabilities
p_X = stats.gamma.pdf(x=x, a=1, loc=1, scale=2)
plt.plot(x,p_X)
plt.xlabel('alpha')
plt.ylabel('P(alpha)')
```

è¾“å‡ºå¦‚ä¸‹æ‰€ç¤º:

![Figure 8.20: Visualization of probabilities
](image/B15019_08_20.jpg)

å›¾ 8.20:æ¦‚ç‡çš„å¯è§†åŒ–

åœ¨å›¾è¡¨ä¸­ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°ğ›¼å€¼è¶Šå°ï¼Œæ¦‚ç‡è¡°å‡å¾—è¶Šå¿«ï¼Œè€Œå€¼è¶Šå¤§ï¼Œè¡°å‡å¾—è¶Šæ…¢ã€‚

éšæœºæœç´¢è¿‡ç¨‹çš„ä¸‹ä¸€æ­¥æ˜¯ä»é€‰å®šçš„åˆ†å¸ƒä¸­æŠ½å– n ä¸ªå€¼ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ç»˜åˆ¶ 100 ä¸ªğ›¼å€¼ã€‚è¯·è®°ä½ï¼Œå¼•å‡ºğ›¼ç‰¹å®šå€¼çš„æ¦‚ç‡ä¸å…¶æ¦‚ç‡ç›¸å…³ï¼Œå¦‚ä»¥ä¸‹åˆ†å¸ƒæ‰€å®šä¹‰:

```
# n sample values
n_iter = 100
# sample from the gamma distribution
samples = stats.gamma.rvs(a=1, loc=1, scale=2, size=n_iter, random_state=100)
```

æ³¨æ„

æˆ‘ä»¬è®¾ç½®äº†ä¸€ä¸ªéšæœºçŠ¶æ€ï¼Œä»¥ç¡®ä¿ç»“æœçš„å¯é‡å¤æ€§ã€‚

ç»˜åˆ¶æ ·æœ¬çš„ç›´æ–¹å›¾ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæ­ç¤ºäº†ä¸€ä¸ªä¸æˆ‘ä»¬é‡‡æ ·çš„åˆ†å¸ƒè¿‘ä¼¼ä¸€è‡´çš„å½¢çŠ¶ã€‚è¯·æ³¨æ„ï¼Œéšç€æ ·æœ¬é‡çš„å¢åŠ ï¼Œç›´æ–¹å›¾è¶Šç¬¦åˆåˆ†å¸ƒ:

```
# visualize the sample distribution
plt.hist(samples)
plt.xlabel('alpha')
plt.ylabel('sample count')
```

è¾“å‡ºå¦‚ä¸‹æ‰€ç¤º:

![Figure 8.21: Visualization of the sample distribution
](image/B15019_08_21.jpg)

å›¾ 8.21:æ ·æœ¬åˆ†å¸ƒçš„å¯è§†åŒ–

ç„¶åï¼Œå°†ä¸ºæ¯ä¸ªğ›¼é‡‡æ ·å€¼æ‹Ÿåˆä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶è¯„ä¼°å…¶æ€§èƒ½ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨æœ¬ç« ä¸­çœ‹åˆ°çš„è¶…å‚æ•°è°ƒæ•´çš„å…¶ä»–æ–¹æ³•ï¼Œæ€§èƒ½å°†ä½¿ç”¨ k å€äº¤å‰éªŒè¯(ä½¿ç”¨`k =10`)è¿›è¡Œè¯„ä¼°ï¼Œä½†æ˜¯å› ä¸ºæˆ‘ä»¬æ­£åœ¨å¤„ç†ä¸€ä¸ªå›å½’é—®é¢˜ï¼Œæ‰€ä»¥æ€§èƒ½åº¦é‡å°†æ˜¯æµ‹è¯•é›†è´Ÿ MSEã€‚

ä½¿ç”¨è¿™ä¸ªåº¦é‡æ„å‘³ç€å€¼è¶Šå¤§è¶Šå¥½ã€‚æˆ‘ä»¬å°†æŠŠç»“æœå­˜å‚¨åœ¨ä¸€ä¸ªå­—å…¸ä¸­ï¼Œæ¯ä¸ªğ›¼å€¼ä½œä¸ºé”®ï¼Œç›¸åº”çš„äº¤å‰éªŒè¯è´Ÿ MSE ä½œä¸ºå€¼:

```
# we will store the results inside a dictionary
result = {}
# for each sample
for sample in samples:

  # initialize a ridge regression estimator with alpha set to the sample value
  reg = linear_model.Ridge(alpha=sample)

  # conduct a 10-fold cross validation scoring on negative mean squared error
  cv = model_selection.cross_val_score(reg, X, y, cv=10, scoring='neg_mean_squared_error')

  # retain the result in the dictionary
  result[sample] = [cv.mean()]
```

æˆ‘ä»¬ä¸æ£€æŸ¥åŸå§‹çš„ç»“æœå­—å…¸ï¼Œè€Œæ˜¯å°†å®ƒè½¬æ¢æˆ pandas æ•°æ®å¸§ï¼Œè½¬ç½®å®ƒï¼Œå¹¶ç»™å‡ºåˆ—åã€‚æŒ‰è´Ÿå‡æ–¹è¯¯å·®é™åºæ’åºæ­ç¤ºäº†è¯¥é—®é¢˜çš„æœ€ä½³æ­£åˆ™åŒ–æ°´å¹³å®é™…ä¸Šæ˜¯å½“ğ›¼çº¦ä¸º 1 æ—¶ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬æ²¡æœ‰æ‰¾åˆ°è¯æ®è¡¨æ˜æ­£åˆ™åŒ–å¯¹äºè¯¥é—®é¢˜æ˜¯å¿…è¦çš„ï¼Œå¹¶ä¸” OLS çº¿æ€§æ¨¡å‹å°†æ»¡è¶³è¦æ±‚:

```
import pandas as pd
# convert the result dictionary to a pandas dataframe, transpose and reset the index
df_result = pd.DataFrame(result).T.reset_index()
# give the columns sensible names
df_result.columns = ['alpha', 'mean_neg_mean_squared_error']
print(df_result.sort_values('mean_neg_mean_squared_error', ascending=False).head())
```

è¾“å‡ºå¦‚ä¸‹æ‰€ç¤º:

![Figure 8.22: Output for the random search process
](image/B15019_08_22.jpg)

å›¾ 8.22:éšæœºæœç´¢è¿‡ç¨‹çš„è¾“å‡º

æ³¨æ„

æ ¹æ®ä½¿ç”¨çš„æ•°æ®ï¼Œç»“æœä¼šæœ‰æ‰€ä¸åŒã€‚

å°½å¯èƒ½å¯è§†åŒ–ç»“æœæ€»æ˜¯æœ‰ç›Šçš„ã€‚é€šè¿‡è´Ÿå‡æ–¹è¯¯å·®å°†ğ›¼ç»˜åˆ¶æˆæ•£ç‚¹å›¾ï¼Œå¯ä»¥æ¸…æ¥šåœ°çœ‹å‡ºï¼Œå†’é™©è¿œç¦»ğ›¼ = 1 ä¸ä¼šå¯¼è‡´é¢„æµ‹æ€§èƒ½çš„æé«˜:

```
plt.scatter(df_result.alpha, df_result.mean_neg_mean_squared_error)
plt.xlabel('alpha')
plt.ylabel('-MSE')
```

è¾“å‡ºå¦‚ä¸‹æ‰€ç¤º:

![Figure 8.23: Plotting the scatter plot
](image/B15019_08_23.jpg)

å›¾ 8.23:ç»˜åˆ¶æ•£ç‚¹å›¾

æˆ‘ä»¬å‘ç°æœ€ä½³ğ›¼ä¸º 1(å…¶é»˜è®¤å€¼)è¿™ä¸€äº‹å®æ˜¯è¶…å‚æ•°è°ƒæ•´ä¸­çš„ä¸€ä¸ªç‰¹ä¾‹ï¼Œå› ä¸ºæœ€ä½³è¶…å‚æ•°åŒ–æ˜¯é»˜è®¤å€¼ã€‚

## ä½¿ç”¨ RandomizedSearchCV è°ƒè°

å®é™…ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ scikit-learn çš„`model_selection`æ¨¡å—ä¸­çš„æ–¹æ³•æ¥è¿›è¡Œæœç´¢ã€‚æ‚¨éœ€è¦åšçš„åªæ˜¯ä¼ é€’æ‚¨çš„ä¼°è®¡é‡ã€æ‚¨å¸Œæœ›è°ƒæ•´çš„è¶…å‚æ•°åŠå…¶åˆ†å¸ƒã€æ‚¨å¸Œæœ›ä»æ¯ä¸ªåˆ†å¸ƒä¸­æŠ½å–çš„æ ·æœ¬æ•°ä»¥åŠæ‚¨å¸Œæœ›ç”¨æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½çš„åº¦é‡ã€‚è¿™äº›åˆ†åˆ«å¯¹åº”äº`param_distributions`ã€`n_iter`å’Œ`scoring`è‡ªå˜é‡ã€‚ä¸ºäº†ä¾¿äºæ¼”ç¤ºï¼Œè®©æˆ‘ä»¬ä½¿ç”¨`RandomizedSearchCV`æ¥æ‰§è¡Œä¹‹å‰å®Œæˆçš„æœç´¢ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åŠ è½½æ•°æ®å¹¶åˆå§‹åŒ–æˆ‘ä»¬çš„å²­å›å½’ä¼°è®¡é‡:

```
from sklearn import datasets, model_selection, linear_model
# load the data
diabetes = datasets.load_diabetes()
# target
y = diabetes.target
# features
X = diabetes.data
# initialise the ridge regression
reg = linear_model.Ridge()
```

ç„¶åï¼Œæˆ‘ä»¬æŒ‡å®šæˆ‘ä»¬æƒ³è¦è°ƒä¼˜çš„è¶…å‚æ•°æ˜¯`alpha`ï¼Œå¹¶ä¸”æˆ‘ä»¬æƒ³è¦ğ›¼åˆ†å¸ƒä¸º`gamma`ï¼Œå…·æœ‰`k = 1`å’Œ`ğœƒ`T4:

```
from scipy import stats
# alpha ~ gamma(1,1)
param_dist = {'alpha': stats.gamma(a=1, loc=1, scale=2)}
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è®¾ç½®å¹¶è¿è¡Œéšæœºæœç´¢æµç¨‹ï¼Œè¯¥æµç¨‹å°†ä»æˆ‘ä»¬çš„`gamma(1,1)`åˆ†å¸ƒä¸­æŠ½å– 100 ä¸ªå€¼ï¼Œæ‹Ÿåˆå²­å›å½’ï¼Œå¹¶ä½¿ç”¨è´Ÿå‡æ–¹è¯¯å·®æŒ‡æ ‡çš„äº¤å‰éªŒè¯è¯„åˆ†æ¥è¯„ä¼°å…¶æ€§èƒ½:

```
# set up the random search to sample 100 values and score on negative mean squared error
rscv = model_selection.RandomizedSearchCV(estimator=reg, param_distributions=param_dist, n_iter=100, scoring='neg_mean_squared_error')
# start the search
rscv.fit(X,y)
```

å®Œæˆæœç´¢åï¼Œæˆ‘ä»¬å¯ä»¥æå–ç»“æœå¹¶ç”Ÿæˆä¸€ä¸ªç†ŠçŒ«æ•°æ®æ¡†æ¶ï¼Œå°±åƒæˆ‘ä»¬ä¹‹å‰æ‰€åšçš„é‚£æ ·ã€‚æŒ‰`rank_test_score`æ’åºå¹¶æŸ¥çœ‹å‰äº”è¡Œä¸æˆ‘ä»¬çš„ç»“è®ºä¸€è‡´ï¼Œå³ alpha åº”è®¾ç½®ä¸º 1ï¼Œæ­£åˆ™åŒ–ä¼¼ä¹ä¸éœ€è¦è§£å†³è¿™ä¸ªé—®é¢˜:

```
import pandas as pd
# convert the results dictionary to a pandas data frame
results = pd.DataFrame(rscv.cv_results_)
# show the top 5 hyperparamaterizations
print(results.loc[:,['params','rank_test_score']].sort_values('rank_test_score').head(5))
```

è¾“å‡ºå¦‚ä¸‹æ‰€ç¤º:

![Figure 8.24: Output for tuning using RandomizedSearchCV
](image/B15019_08_24.jpg)

å›¾ 8.24:ä½¿ç”¨ RandomizedSearchCV è¿›è¡Œè°ƒä¼˜çš„è¾“å‡º

æ³¨æ„

æ ¹æ®æ—¥æœŸçš„ä¸åŒï¼Œä¸Šè¿°ç»“æœå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒã€‚

## ç»ƒä¹  8.03:éšæœºæ£®æ—åˆ†ç±»å™¨çš„éšæœºæœç´¢è¶…å‚æ•°è°ƒæ•´

åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œæˆ‘ä»¬å°†å†æ¬¡è®¨è®ºæ‰‹å†™æ•°å­—åˆ†ç±»é—®é¢˜ï¼Œè¿™æ¬¡ä½¿ç”¨éšæœºæ£®æ—åˆ†ç±»å™¨ï¼Œå…¶è¶…å‚æ•°é€šè¿‡éšæœºæœç´¢ç­–ç•¥è¿›è¡Œè°ƒæ•´ã€‚éšæœºæ£®æ—æ˜¯ç”¨äºå•ç±»å’Œå¤šç±»åˆ†ç±»é—®é¢˜çš„æµè¡Œæ–¹æ³•ã€‚å®ƒé€šè¿‡ç”Ÿé•¿ç®€å•çš„æ ‘æ¨¡å‹æ¥å­¦ä¹ ï¼Œæ¯ä¸ªæ¨¡å‹éƒ½é€æ­¥å°†æ•°æ®é›†åˆ†å‰²æˆèƒ½å¤Ÿæœ€å¥½åœ°åˆ†ç¦»ä¸åŒç±»çš„ç‚¹çš„åŒºåŸŸã€‚

äº§ç”Ÿçš„æœ€ç»ˆæ¨¡å‹å¯ä»¥è¢«è®¤ä¸ºæ˜¯ n ä¸ªæ ‘æ¨¡å‹ä¸­æ¯ä¸€ä¸ªçš„å¹³å‡å€¼ã€‚è¿™æ ·ï¼Œéšæœºæ£®æ—å°±æ˜¯ä¸€ä¸ª`ensemble`æ–¹æ³•ã€‚åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œæˆ‘ä»¬å°†è°ƒæ•´çš„å‚æ•°æ˜¯`criterion`å’Œ`max_features`ã€‚

`criterion`æ˜¯æŒ‡ä»ç±»åˆ«çº¯åº¦çš„è§’åº¦è¯„ä¼°æ¯ä¸ªåˆ†è£‚çš„æ–¹å¼(åˆ†è£‚è¶Šçº¯è¶Šå¥½),è€Œ`max_features`æ˜¯éšæœºæ£®æ—åœ¨å¯»æ‰¾æœ€ä½³åˆ†è£‚æ—¶å¯ä»¥ä½¿ç”¨çš„æœ€å¤§ç‰¹å¾æ•°ã€‚

æ³¨æ„

è¿™ä¸ªç»ƒä¹ çš„ä»£ç å¯ä»¥åœ¨ https://packt.live/2uDVct8 æ‰¾åˆ°ã€‚

ä»¥ä¸‹æ­¥éª¤å°†å¸®åŠ©æ‚¨å®Œæˆç»ƒä¹ ã€‚

1.  åœ¨ Google Colab ä¸­åˆ›å»ºæ–°ç¬”è®°æœ¬ã€‚
2.  å¯¼å…¥æ•°æ®å¹¶åˆ†ç¦»ç‰¹å¾`X`å’Œç›®æ ‡`y` :

    ```
    from sklearn import datasets
    # import data
    digits = datasets.load_digits()
    # target
    y = digits.target
    # features
    X = digits.data
    ```

3.  åˆå§‹åŒ–éšæœºæ£®æ—åˆ†ç±»å™¨ä¼°è®¡å™¨ã€‚æˆ‘ä»¬å°†æŠŠ`n_estimators`è¶…å‚æ•°è®¾ç½®ä¸º`100`ï¼Œè¿™æ„å‘³ç€æœ€ç»ˆæ¨¡å‹çš„é¢„æµ‹æœ¬è´¨ä¸Šå°†æ˜¯`100`ç®€å•æ ‘æ¨¡å‹çš„å¹³å‡å€¼ã€‚æ³¨æ„ä½¿ç”¨éšæœºçŠ¶æ€ä»¥ç¡®ä¿ç»“æœçš„å¯å†ç°æ€§:

    ```
    from sklearn import ensemble
    # an ensemble of 100 estimators
    rfc = ensemble.RandomForestClassifier(n_estimators=100, random_state=100)
    ```

4.  One of the parameters we will be tuning is `max_features`. Let's find out the maximum value this could take:

    ```
    # how many features do we have in our dataset?
    n_features = X.shape[1]
    print(n_features)
    ```

    æ‚¨åº”è¯¥ä¼šçœ‹åˆ°æˆ‘ä»¬æœ‰ 64 ä¸ªåŠŸèƒ½:

    ```
    64
    ```

    ç°åœ¨æˆ‘ä»¬çŸ¥é“äº†`max_features`çš„æœ€å¤§å€¼ï¼Œæˆ‘ä»¬å¯ä»¥è‡ªç”±åœ°å®šä¹‰éšæœºåŒ–æœç´¢è¿‡ç¨‹çš„è¶…å‚æ•°è¾“å…¥ã€‚åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œæˆ‘ä»¬æ²¡æœ‰ç†ç”±ç›¸ä¿¡`max_features`çš„ä»»ä½•ç‰¹å®šå€¼æ˜¯æ›´ä¼˜çš„ã€‚

5.  è®¾ç½®è¦†ç›–èŒƒå›´`1`åˆ°`64`çš„ç¦»æ•£å‡åŒ€åˆ†å¸ƒã€‚è®°ä½è¿™ä¸ªåˆ†å¸ƒçš„æ¦‚ç‡è´¨é‡å‡½æ•°ï¼Œ`P(X=x) = 1/n`ï¼Œæ‰€ä»¥åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­æ˜¯`P(X=x) = 1/64`ã€‚å› ä¸º`criterion`åªæœ‰ä¸¤ä¸ªç¦»æ•£é€‰é¡¹ï¼Œè¿™ä¹Ÿå°†ä½œä¸ºç¦»æ•£å‡åŒ€åˆ†å¸ƒç”¨`P(X=x) = Â½` :

    ```
    from scipy import stats
    # we would like to smaple from criterion and max_features as discrete uniform distributions
    param_dist = {
        'criterion': ['gini', 'entropy'],
        'max_features': stats.randint(low=1, high=n_features)
    }
    ```

    é‡‡æ ·
6.  æˆ‘ä»¬ç°åœ¨å·²ç»æœ‰äº†è®¾ç½®éšæœºæœç´¢è¿‡ç¨‹æ‰€éœ€çš„ä¸€åˆ‡ã€‚å’Œä»¥å‰ä¸€æ ·ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å‡†ç¡®æ€§ä½œä¸ºæ¨¡å‹è¯„ä¼°çš„åº¦é‡ã€‚æ³¨æ„éšæœºçŠ¶æ€çš„ä½¿ç”¨:

    ```
    from sklearn import model_selection
    # setting up the random search sampling 50 times and conducting 5-fold cross-validation
    rscv = model_selection.RandomizedSearchCV(estimator=rfc, param_distributions=param_dist, n_iter=50, cv=5, scoring='accuracy' , random_state=100)
    ```

7.  Let's kick off the process with the. `fit` method. Please note that both fitting random forests and cross-validation are computationally expensive processes due to their internal processes of iteration. Generating a result may take some time:

    ```
    # start the process
    rscv.fit(X,y)
    ```

    æ‚¨åº”è¯¥çœ‹åˆ°ä»¥ä¸‹å†…å®¹:

    ![Figure 8.25: RandomizedSearchCV results
    ](image/B15019_08_25.jpg)

    å›¾ 8.25:éšæœºæœç´¢ç»“æœ

8.  Next, you need to examine the results. Create a `pandas` DataFrame from the `results` attribute, order by the `rank_test_score`, and look at the top five model hyperparameterizations. Note that because the random search draws samples of hyperparameterizations at random, it is possible to have duplication. We remove the duplicate entries from the DataFrame:

    ```
    import pandas as pd
    # convert the dictionary of results to a pandas dataframe
    results = pd.DataFrame(rscv.cv_results_)
    # removing duplication
    distinct_results = results.loc[:,['params','mean_test_score']]
    # convert the params dictionaries to string data types
    distinct_results.loc[:,'params'] = distinct_results.loc[:,'params'].astype('str')
    # remove duplicates
    distinct_results.drop_duplicates(inplace=True)
    # look at the top 5 best hyperparamaterizations
    distinct_results.sort_values('mean_test_score', ascending=False).head(5)
    ```

    æ‚¨åº”è¯¥å¾—åˆ°ä»¥ä¸‹è¾“å‡º:

    ![Figure 8.26: Top five hyperparameterizations
    ](image/B15019_08_26.jpg)

    å›¾ 8.26:äº”å¤§è¶…å‚æ•°åŒ–

9.  The last step is to visualize the result. Including every parameterization will result in a cluttered plot, so we will filter on parameterizations that resulted in a mean test score > 0.93:

    ```
    # top performing models
    distinct_results[distinct_results.mean_test_score > 0.93].sort_values('mean_test_score').plot.barh(x='params', xlim=(0.9))
    ```

    è¾“å‡ºå¦‚ä¸‹æ‰€ç¤º:

![Figure 8.27: Visualizing the test scores of the top-performing models
](image/B15019_08_27.jpg)

å›¾ 8.27:å¯è§†åŒ–è¡¨ç°æœ€å¥½çš„æ¨¡å‹çš„æµ‹è¯•åˆ†æ•°

æˆ‘ä»¬å‘ç°æœ€å¥½çš„è¶…å‚æ•°åŒ–æ˜¯ä½¿ç”¨æœ€å¤§ç‰¹å¾è®¾ç½®ä¸º 4 çš„`gini`æ ‡å‡†çš„éšæœºæ£®æ—åˆ†ç±»å™¨ã€‚

## éšæœºæœç´¢çš„ä¼˜ç‚¹å’Œç¼ºç‚¹

å› ä¸ºéšæœºæœç´¢ä»ä¸€ç³»åˆ—å¯èƒ½çš„è¶…å‚æ•°åŒ–ä¸­æŠ½å–æœ‰é™çš„æ ·æœ¬(`model_selection.RandomizedSearchCV`ä¸­çš„`n_iter`)ï¼Œæ‰€ä»¥å°†è¶…å‚æ•°æœç´¢çš„èŒƒå›´æ‰©å±•åˆ°ç½‘æ ¼æœç´¢çš„å®é™…èŒƒå›´ä¹‹å¤–æ˜¯å¯è¡Œçš„ã€‚è¿™æ˜¯å› ä¸ºç½‘æ ¼æœç´¢å¿…é¡»å°è¯•èŒƒå›´å†…çš„æ‰€æœ‰å†…å®¹ï¼Œè®¾ç½®å¤§èŒƒå›´çš„å€¼å¯èƒ½ä¼šå¤ªæ…¢è€Œæ— æ³•å¤„ç†ã€‚æœç´¢è¿™ä¸ªæ›´å¤§çš„èŒƒå›´ç»™ä½ æœºä¼šå‘ç°ä¸€ä¸ªçœŸæ­£çš„æœ€ä¼˜è§£ã€‚

ä¸æ‰‹åŠ¨å’Œç½‘æ ¼æœç´¢ç­–ç•¥ç›¸æ¯”ï¼Œæ‚¨ç¡®å®ç‰ºç‰²äº†ä¸€å®šç¨‹åº¦çš„æ§åˆ¶æ¥è·å¾—è¿™ç§å¥½å¤„ã€‚å¦ä¸€ä¸ªè€ƒè™‘æ˜¯ï¼Œè®¾ç½®éšæœºæœç´¢æ¯”å…¶ä»–é€‰é¡¹æ›´å¤æ‚ï¼Œå› ä¸ºæ‚¨å¿…é¡»æŒ‡å®šåˆ†å¸ƒã€‚æ€»æœ‰å‡ºé”™çš„å¯èƒ½ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœä½ ä¸ç¡®å®šä½¿ç”¨ä»€ä¹ˆæ ·çš„åˆ†å¸ƒï¼ŒåšæŒå¯¹å„è‡ªçš„å˜é‡ç±»å‹ä½¿ç”¨ç¦»æ•£æˆ–è¿ç»­å‡åŒ€åˆ†å¸ƒï¼Œå› ä¸ºè¿™å°†ä¸ºæ‰€æœ‰é€‰é¡¹åˆ†é…ç›¸åŒçš„é€‰æ‹©æ¦‚ç‡ã€‚

## æ´»åŠ¨ 8.01:è˜‘è‡æœ‰æ¯’å—ï¼Ÿ

å‡è®¾ä½ æ˜¯ä¸€åæ•°æ®ç§‘å­¦å®¶ï¼Œåœ¨å½“åœ°å¤§å­¦çš„ç”Ÿç‰©ç³»å·¥ä½œã€‚ä½ çš„åŒäº‹æ˜¯ä¸€åçœŸèŒå­¦å®¶(ä¸€åä¸“é—¨ç ”ç©¶çœŸèŒçš„ç”Ÿç‰©å­¦å®¶)ï¼Œå¥¹è¯·æ±‚ä½ å¸®åŠ©å¥¹å¼€å‘ä¸€ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«ç‰¹å®šè˜‘è‡ç‰©ç§æ˜¯å¦æœ‰æ¯’ï¼Œæˆ–è€…æ˜¯å¦å…·æœ‰ä¸å…¶å¤–è§‚ç›¸å…³çš„å±æ€§ã€‚

è¿™é¡¹æ´»åŠ¨çš„ç›®æ ‡æ˜¯é‡‡ç”¨ç½‘æ ¼å’Œéšæœºæœç´¢ç­–ç•¥æ¥æ‰¾åˆ°ä¸€ä¸ªæœ€ä½³æ¨¡å‹ã€‚

æ³¨æ„

æœ¬ç»ƒä¹ ä¸­ä½¿ç”¨çš„æ•°æ®é›†å¯ä»¥åœ¨æˆ‘ä»¬ä½äº https://packt.live/38zdhaB çš„ GitHub å­˜å‚¨åº“ä¸­æ‰¾åˆ°ã€‚

å…³äºæ•°æ®é›†å±æ€§çš„è¯¦ç»†ä¿¡æ¯å¯ä»¥åœ¨åŸå§‹æ•°æ®é›†ç«™ç‚¹ä¸Šæ‰¾åˆ°:[https://packt.live/36j0jfA](https://packt.live/36j0jfA)ã€‚

1.  Load the data into Python using the `pandas.read_csv()` method, calling the object `mushrooms`.

    æç¤º:æ•°æ®é›†æ˜¯ CSV æ ¼å¼ï¼Œæ²¡æœ‰æ ‡é¢˜ã€‚åœ¨`pandas.read_csv()`ä¸­è®¾ç½®`header=None`ã€‚

2.  Separate the target, `y` and features, `X` from the dataset.

    æç¤º:å¯ä»¥åœ¨ç¬¬ä¸€åˆ—(`mushrooms.iloc[:,0]`)ä¸­æ‰¾åˆ°ç›®æ ‡ï¼Œåœ¨å…¶ä½™åˆ—(`mushrooms.iloc[:,1:]`)ä¸­æ‰¾åˆ°ç‰¹å¾ã€‚

3.  å¯¹ç›®æ ‡`y`é‡æ–°ç¼–ç ï¼Œä½¿æ¯’è˜‘è‡è¡¨ç¤ºä¸º`1`ï¼Œé£Ÿç”¨èŒè¡¨ç¤ºä¸º`0`ã€‚
4.  Transform the columns of the featureset `X` into a `numpy` array with a binary representation. This is known as one-hot encoding.

    æç¤º:ä½¿ç”¨`preprocessing.OneHotEncoder()`æ¥å˜æ¢`X`ã€‚

5.  Conduct both a grid and random search to find an optimal hyperparameterization for a random forest classifier. Use accuracy as your method of model evaluation. Make sure that when you initialize the classifier and when you conduct your random search, `random_state = 100`.

    å¯¹äºç½‘æ ¼æœç´¢ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹å†…å®¹:

    ```
    {
        'criterion': ['gini', 'entropy'],
        'max_features': [2, 4, 6, 8, 10, 12, 14]
    }
    ```

    å¯¹äºéšæœºæœç´¢ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹å†…å®¹:

    ```
    {
        'criterion': ['gini', 'entropy'],
        'max_features': stats.randint(low=1, high=max_features)
    }
    ```

6.  Plot the mean test score versus hyperparameterization for the top 10 models found using random search.

    æ‚¨åº”è¯¥ä¼šçœ‹åˆ°ç±»ä¼¼ä¸‹é¢çš„å›¾:

![Figure 8.28: Mean test score plot
](image/B15019_08_28.jpg)

å›¾ 8.28:å¹³å‡æµ‹è¯•åˆ†æ•°å›¾

æ³¨æ„

æ´»åŠ¨çš„è§£å†³æ–¹æ¡ˆå¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°:[https://packt.live/2GbJloz](https://packt.live/2GbJloz)ã€‚

# æ€»ç»“

åœ¨è¿™ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†ä¸‰ç§è¶…å‚æ•°è°ƒæ•´ç­–ç•¥ï¼Œè¿™ä¸‰ç§ç­–ç•¥éƒ½æ˜¯åŸºäºå¯¹æé«˜æ€§èƒ½çš„ä¼°è®¡å™¨è¶…å‚æ•°åŒ–çš„æœç´¢ã€‚

æ‰‹åŠ¨æœç´¢æ˜¯ä¸‰ç§æœç´¢ä¸­æœ€éœ€è¦åŠ¨æ‰‹çš„ï¼Œä½†å®ƒç»™ä½ ä¸€ç§ç‹¬ç‰¹çš„æ„Ÿè§‰ã€‚å®ƒé€‚ç”¨äºæ‰€è®¨è®ºçš„ä¼°è®¡é‡å¾ˆç®€å•çš„æƒ…å†µ(è¶…å‚æ•°æ•°é‡å¾ˆå°‘)ã€‚

ç½‘æ ¼æœç´¢æ˜¯ä¸€ç§è‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œæ˜¯ä¸‰ç§æ–¹æ³•ä¸­æœ€ç³»ç»Ÿçš„ï¼Œä½†æ˜¯å½“å¯èƒ½çš„è¶…å‚æ•°åŒ–èŒƒå›´å¢åŠ æ—¶ï¼Œè¿è¡Œèµ·æ¥è®¡ç®—é‡éå¸¸å¤§ã€‚

éšæœºæœç´¢è™½ç„¶è®¾ç½®èµ·æ¥æœ€å¤æ‚ï¼Œä½†å®ƒæ˜¯åŸºäºè¶…å‚æ•°åˆ†å¸ƒçš„é‡‡æ ·ï¼Œè¿™å…è®¸æ‚¨æ‰©å¤§æœç´¢èŒƒå›´ï¼Œä»è€Œè®©æ‚¨æœ‰æœºä¼šå‘ç°ç½‘æ ¼æˆ–æ‰‹åŠ¨æœç´¢é€‰é¡¹å¯èƒ½ä¼šé”™è¿‡çš„å¥½è§£å†³æ–¹æ¡ˆã€‚åœ¨ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹å¦‚ä½•å¯è§†åŒ–ç»“æœï¼Œæ€»ç»“æ¨¡å‹ï¼Œå¹¶é˜æ˜ç‰¹å¾çš„é‡è¦æ€§å’Œæƒé‡ã€‚